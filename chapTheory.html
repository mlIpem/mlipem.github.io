<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Theory | Exploring music interactions</title>
<meta name="author" content="Marc Leman">
<meta name="description" content="All the assumptions – or call them hypotheses – that will be tested in this book refer to a theory of music interaction. This theory is based on something that our grandmothers told us1, namely...">
<meta name="generator" content="bookdown 0.41 with bs4_book()">
<meta property="og:title" content="Chapter 1 Theory | Exploring music interactions">
<meta property="og:type" content="book">
<meta property="og:url" content="https://www.ugent.be/chapTheory.html">
<meta property="og:image" content="https://www.ugent.be/images/cover.jpeg">
<meta property="og:description" content="All the assumptions – or call them hypotheses – that will be tested in this book refer to a theory of music interaction. This theory is based on something that our grandmothers told us1, namely...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Theory | Exploring music interactions">
<meta name="twitter:description" content="All the assumptions – or call them hypotheses – that will be tested in this book refer to a theory of music interaction. This theory is based on something that our grandmothers told us1, namely...">
<meta name="twitter:image" content="https://www.ugent.be/images/cover.jpeg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Lato-0.4.9/font.css" rel="stylesheet">
<link href="libs/Roboto_Mono-0.4.9/font.css" rel="stylesheet">
<link href="libs/Montserrat-0.4.9/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.8.0/transition.js"></script><script src="libs/bs3compat-0.8.0/tabs.js"></script><script src="libs/bs3compat-0.8.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-99618359-1', 'auto');
      ga('send', 'pageview');

    </script><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-CDTTJLM7N4"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-CDTTJLM7N4');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">Exploring music interactions</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="active" href="chapTheory.html"><span class="header-section-number">1</span> Theory</a></li>
<li><a class="" href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></li>
<li><a class="" href="chapListener.html"><span class="header-section-number">3</span> Listener appreciation</a></li>
<li><a class="" href="chapDancer.html"><span class="header-section-number">4</span> Dancer synchronization</a></li>
<li><a class="" href="chapViolinist.html"><span class="header-section-number">5</span> Violin player visual feedback</a></li>
<li><a class="" href="chapExoskeletons.html"><span class="header-section-number">6</span> Two violin players’ haptic feedback</a></li>
<li><a class="" href="chapTappers1.html"><span class="header-section-number">7</span> Two finger tappers’ entrainment (Part 1)</a></li>
<li><a class="" href="chapTappers2.html"><span class="header-section-number">8</span> Two finger tappers’ entrainment (Part 2)</a></li>
<li><a class="" href="chapConclusion.html"><span class="header-section-number">9</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mlIpem/ExploringMusicInteractionWithR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapTheory" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Theory<a class="anchor" aria-label="anchor" href="#chapTheory"><i class="fas fa-link"></i></a>
</h1>
<p>All the assumptions – or call them hypotheses – that will be tested in this book refer to a theory of music interaction. This theory is based on something that our grandmothers told us<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Admittedly, not only grandmothers. Also great writers and philosophers, including Homer in the &lt;em&gt;Iliad&lt;/em&gt;, Dante in the &lt;em&gt;Divina Comedia&lt;/em&gt;, and Spinoza in the &lt;em&gt;Ethica.&lt;/em&gt; And many others, including painters!&lt;/p&gt;"><sup>1</sup></a>, namely that to achieve something in life, you have to improve yourself through physical effort and endurance.</p>
<p>Applied to music, this means that we need an alert and active engagement to gain something from music, such as a feeling of beauty or power. Musicians may recognize this; good performances build on precision and focus.</p>
<p>Now, this chapter is an attempt to reformulate our grandmothers’ advice from the perspective of modern music research.</p>
<div id="self-augmented-interactions" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Self-augmented interactions<a class="anchor" aria-label="anchor" href="#self-augmented-interactions"><i class="fas fa-link"></i></a>
</h2>
<p>In this book, we assume that people have an urge to engage in particular interactions with music because they derive benefits from doing so. For example, they discover new musical features, they improve the accuracy of their movements in dance, they perform better in an ensemble, and they experience positive emotions. The benefits experienced relate to learning, self-improvement, and enriching experiences. Alertness, attention, and focus are crucial elements for reaching the level at which interaction is experienced as beneficial. Unlike taking drugs, the approach is not passive; it is quite the opposite. It requires physical effort and endurance to experience the benefits and become aware of them.</p>
<p>In this book, we discuss <em>self-augmented interactions</em>, as people tend to create these interactions voluntarily, through choice and decision. Through these interactions, they augment themselves, performing better and experiencing beneficial long-term effects (over days, weeks, and beyond). Our claim is that these long-term effects are conditional upon self-augmented interactions with music. In other words, self-augmented interactions are necessary for these effects to occur. However, due to confounding variables, it may not be easy to predict these effects.</p>
<p>Currently, we are far from being able to predict the long-term effects of music, as the research domain is still young and not yet mature enough to address the enormous complexity of the subject. In their comprehensive review of the evidence for the power of music, Hallam and Himonides conclude: “As the evidence stands at the moment, it is not clear which
musical activities might be beneficial for promoting any particular
outcome. Currently, much of the evidence is mixed, to some extent
explicable in terms of the different musical activities studied and the
research methods used. As research into the wider benefits of music is
not a high priority for research funders, it may be many years before it
is possible to provide clear guidance on which musical interventions
might lead to particular outcomes, and what qualities in the delivery of
those interventions are key to success.” (p. 595)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See Hallam and Himonides (2022).
Note that references in this book are restricted, mainly to reviews and textbooks, but not exclusively. &lt;/p&gt;"><sup>2</sup></a></p>
<p>Given the interaction and its involvement of bodily functions, we envision a complex system whose states can be built up over time, maintained for a period, and then decline back to normal. Rather than focusing on the off-line effects of music, such as what happens after listening, dancing, or playing music, we are interested in examining the on-line effects—that is, what happens during the interaction with music. This is a relatively new domain, and as we have mentioned, understanding on-line effects is a prerequisite for comprehending off-line effects. Therefore, our primary focus should be on understanding what happens during the interaction with music.</p>
<p>Evidence can be drawn from behavioral and neuroscience studies, with neurobiology playing a necessary role in the complete picture. Indeed, increased alertness, reduced stress, pleasure, and motivation suggest the involvement of neurotransmitters and hormones such as norepinephrine, oxytocin, cortisol, and dopamine, which regulate brain functions and affect behavioral activities during interactions<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For a review of neurochemical responses to music, see Reybrouck and Van Dyck (2024).&lt;/p&gt;"><sup>3</sup></a>.</p>
<p>However, this book has a rather focused scope. Our contribution to this theory centers primarily on timing performances. Additionally, we concentrate on the statistical processing of timing data.</p>
<p>Consequently, what we aim to achieve with this book is a small part of the grand dynamics at play. Therefore, our claims are modest.</p>
<p>We believe that timing is interesting because it is a distinctive, measurable, and partly controllable feature of interaction. As mentioned, we do not focus on the after-effects of timing or neuroplasticity effects. Instead, we return to the basics and study fundamental aspects using statistical processing tools. We examine the preconditions for plausible effects in performance parameters, aiming to better understand the dynamics of music interaction.</p>
<p>A paradigmatic example is the musical tempo played by an ensemble of musicians. As a feature of timing, it is created and maintained through collaborative effort, involving the co-regulation of individual musicians’ actions within the ensemble. Tempo is a delicate state associated with the ensemble’s musical quality. A stable tempo is crucial, except when changes in tempo are required. While tempo is not the only interesting feature of timing, it is certainly important. It can be measured, for instance, by identifying onsets in audio and calculating the underlying distribution of inter-onset intervals. When the intended stable timing is achieved, the ensemble can be said to be in a special interaction state.</p>
<p>The co-regulation of actions to maintain an intended stable tempo requires alertness, physical effort, and fine sensory-motor control from each member. Any mistake can disrupt the moment. However, the effort pays off: a good performance brings relief and satisfaction.</p>
<p>In short, the theory of self-augmented interaction is about improving ourselves with music, through performing. It requires an optimal performance of action and perception processes, it involves effort – pleasure – reward – motivation, and it draws upon intentions, goals and collaborative actions.
While a complete theory of <em>self-augmented interaction</em> might not yet be within our reach, our goal is to contribute to it, firstly by proposing elements that contribute to theory, secondly, and perhaps more important for this book, by offering data-analysis tools that sharpen some details of contributing insights that may lead us towards such theory. Thereby, we don’t focus on effects of music, but rather on properties of the interaction.</p>
<p>In short, the theory of self-augmented interaction is about improving ourselves through musical performance. It requires optimal integration of action and perception processes, involving effort, pleasure, reward, and motivation, and it draws upon intentions, goals, and collaborative actions. While a complete theory of self-augmented interaction may not yet be within our reach, our goal is to contribute to it in two main ways: first, by proposing elements that contribute to the theory; and second, more importantly for this book, by offering data-analysis tools that refine our understanding of key insights and guide us toward such a theory. To do this, we focus not on the effects of music, but rather on the properties of the interaction itself.</p>
</div>
<div id="supporting-theories" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Supporting theories<a class="anchor" aria-label="anchor" href="#supporting-theories"><i class="fas fa-link"></i></a>
</h2>
<p>The ultimate theory draws upon a number of other theories about humans interacting with their environment.
Those theories relate to embodiment, prediction and expression.</p>
<div id="embodiment" class="section level3 unnumbered">
<h3>Embodiment<a class="anchor" aria-label="anchor" href="#embodiment"><i class="fas fa-link"></i></a>
</h3>
<p><em>Embodiment</em> holds the idea that humans have a repertoire of gestures and actions, which they use for coding and decoding music<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;These ideas have been explored in several books, such as: Leman (2007, 2016), as well as several book chapters in Godøy and Leman, eds. (2010) and Bader, ed. (2028).&lt;/p&gt;"><sup>4</sup></a>.</p>
<ul>
<li><p>During musical performance (or coding), various bodily gestures shape the phrasing, articulation, intonation, and overall structure of the sound. Consequently, the musical sound reflects traces of these gestures as patterns, known as <em>moving sonic forms</em>.</p></li>
<li><p>When listening and dancing (or decoding), individuals draw upon their own repertoire of bodily gestures to align with these moving sonic forms. This alignment enables them to experience music from a gestural and action-oriented perspective, essentially embodying the gestural traces within the music.</p></li>
</ul>
<p>Coding and decoding can be understood as components of an <em>expressive communication process</em>. Coding translates gestures into sound, while decoding converts sound back into gestures. In communication, people share a common repertoire of information, which is conveyed through gestures and actions. This repertoire is both biological and cultural in nature. It is biological because people share a similar biological and biomechanical foundation. It is cultural due to the codified ways people move and behave within specific cultural contexts. Therefore, this shared repertoire enables individuals to encode and decode information through music using their biological and culturally codified gestures. In the context of communication, music serves as a vehicle for mediating gestures and actions that carry expressive forms.</p>
<p>Consider the role of timing. As bodily actions generate sound cues for timing, there is an intrinsic relationship between timing and body movement. Timing is encoded into sound and can be decoded from sound, allowing us to experience and perceive the timing. Consequently, sound or music serves as a vehicle for mediating timing gestures, which carry expressive forms that can be experienced and interpreted.</p>
<p>The link between sound and gestures can be crucial for the measurement of timing, even though detecting timing in sound can be challenging. Consider, for example, the onsets of violin tones and the timing they induce. It can be difficult to detect timing cues from violin audio alone. However, it is likely that these timing cues can be extracted from the violin bowing gestures, as the movement goes from point A to point B in space. The turning points at A and B may provide important cues for timing. This is measured not with sound recording, but with movement recording, using a <em>motion capture system</em><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This technology, along with many others, is standard in music research. An example infrastructure can be explored at &lt;a href="https://asil.ugent.be/infrastructure/" class="uri"&gt;https://asil.ugent.be/infrastructure/&lt;/a&gt;.&lt;/p&gt;'><sup>5</sup></a>.</p>
</div>
<div id="prediction" class="section level3 unnumbered">
<h3>Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h3>
<p>Embodiment is closely linked with <em>prediction</em>, as moving sonic forms can be anticipated by the human encoder/decoder (Seth, 2015). Often, this anticipation is based on gesturing. Interestingly, during dancing to the beat of music, gestural alignment and body movement are typically initiated ahead of the beat. If not initiated ahead, the crucial timing movement would come too late.</p>
<p>Moving in sync with the musical beat demonstrates that embodiment is fundamentally predictive. Consider conducting your favorite piece of music on CD<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Admittedly, this is old technology, but it also applies to streaming&lt;/p&gt;"><sup>6</sup></a>. Your conducting is likely ahead of the music due to your brain’s predictive processing. When deeply engaged in conducting, it probably evokes a feeling of agency and power. Why?</p>
<p>This is due to the <em>reverse causality illusion</em><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;David Hume, in his &lt;em&gt;A Treatise of Human Nature&lt;/em&gt;, launched the idea that causality in the brain is a matter of association, taking into account time and a contiguous relationship between cause and effect. In other words, Hume assumed that in the brain, causality is just a association, nothing more, no force or whatsoever.&lt;/p&gt;"><sup>7</sup></a>. In this illusion, it appears as if your gestures cause the music—as if your movements are the cause and the music is the effect. The reason for this illusion is the correlation and contiguity between gestural and musical events: gestural events precede musical events.</p>
<p>Clearly, the perception of reverse causality is illusory, as the music continues even if your gesturing suddenly stops. Nevertheless, experiencing this illusion may create a strong feeling of control (agency). You may feel that your gestures influence the music, providing a sense of power and satisfaction. These effects occur during the musical interaction and performance. Whether these effects persist after the performance is another matter.</p>
<p>The connection to self-augmented interaction is evident. Establishing reverse causality creates a self-augmented interaction state. The ability to generate this state is crucial for the human decoder to create empowering effects.</p>
<p>However, reverse causality is not the only effect. Prediction enables the co-regulation of one’s actions with those heard from other musicians, allowing for the establishment of a stable tempo through collaboration. Prediction is ubiquitous and thus a crucial element in understanding how self-augmented interaction states are maintained. Effects such as the reverse causality illusion can be seen as consequences of prediction.</p>
</div>
<div id="expression" class="section level3 unnumbered">
<h3>Expression<a class="anchor" aria-label="anchor" href="#expression"><i class="fas fa-link"></i></a>
</h3>
<p>Embodiment and prediction, when considered from the perspective of social interaction, support <em>expression</em>. Gestures serve as social signals, grounded in human biology and culturally codified social actions. A gesture becomes an expressive signal when the receiver responds with an expressive signal <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The viewpoint adopted here is based on scholars working on human interaction, such as David Hume, Charles Darwin, Adam Smith, Erving Goffman. See Bonicco-Donato (2016) for a historical overview.&lt;/p&gt;"><sup>8</sup></a>. Through the exchange of expressive signals via gestures, the sender and receiver typically trigger embodied predictions, arousal, enhanced attention, focus, and emotional engagement. This exchange can lead to ritualized behavior when gestures become habituated.</p>
<p>Expression is thus vital in human interactions. <em>Expressive timing</em> refers to the use of timing as a signal that evokes responses from the audience. Through the mutual exchange of gestural expressions and predictions, humans can interact in a coordinated manner. Musical rhythms enhance these exchanges of bodily expressions, facilitating the formation of co-regulated interactions and paving the way for self-augmentation.</p>
<p>Together, the three constructs—gestural, anticipatory, and expressive dynamics—provide a framework for understanding musical self-augmentation. This dynamic ultimately leads to an enhanced state where senses, cognitive abilities, and emotional engagement are sharpened and function optimally. We are particularly interested in the behavioral features of this dynamic as expressed through timing. Although this is a specific domain of musical interaction, it is both challenging and intriguing.</p>
</div>
</div>
<div id="music-as-moving-sonic-forms" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Music as moving sonic forms<a class="anchor" aria-label="anchor" href="#music-as-moving-sonic-forms"><i class="fas fa-link"></i></a>
</h2>
<p>Given the above theories about human interactive abilities, it is valuable to examine music itself, particularly how music can be understood as moving sonic forms. This perspective is based on the notions of <em>pattern emergence</em> and <em>endowed expression</em>, which are vital for understanding interaction.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The concepts were introduced in Leman (2008) but it is of interest to briefly summarize them here.&lt;/p&gt;"><sup>9</sup></a></p>
<div id="pattern-emergence" class="section level3 unnumbered">
<h3>Pattern emergence<a class="anchor" aria-label="anchor" href="#pattern-emergence"><i class="fas fa-link"></i></a>
</h3>
<p><em>Pattern emergence</em> is the concept that a pattern possesses structural features aligned with the human capacity for emergence. This alignment, along with the pattern’s structure, facilitates the emergence of perceptual phenomena. Consider the auditory system: it has the capacity to transform a pattern with specific structural features into a perceptual experience. For instance, a harmonic structure composed of 600, 800, 1000, and 1200 Hz may be perceived as a pitch at 200 Hz due to the auditory system’s transformative ability<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For a review of the human auditory system and pitch as pattern emergence, see (Langner, 2015).&lt;/p&gt;"><sup>10</sup></a>.. The pattern is perceived as such because its structure allows for this pitch perception, and the auditory system performs this transformation.</p>
<p>The auditory system can also integrate multiple harmonic patterns into a single perceived chord. When such harmonic patterns are played in sequence, their integration may generate expectations, leading to tonal tensions and relaxation dynamics<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For reviews of pitch and timbre perception in music, see Schneider (2018a, b).&lt;/p&gt;"><sup>11</sup></a>. Furthermore, the auditory system can process slightly altered structures, resulting in shifts in pitch perception.</p>
<p>Interestingly, the bottom-up mechanisms of pattern emergence may compete with top-down mechanisms driven by habitual patterns. This competition can influence the perception of tonal tension and relaxation. While the precise contributions of sensory (bottom-up) and cognitive (long-term memory) processing are still debated <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Collins et al. (2014) show how tonality can be understood from sensory processing.
Sears et al. (2018) for a discussion of sensory and cognitive tonality effects.&lt;/p&gt;"><sup>12</sup></a>, there is a considerable understanding of these mechanisms. Pitch emergence arises from the structure of patterns and underlying dispositions, involving auditory and brain mechanisms.</p>
<p>A similar observation can be made for rhythms. Rhythms consist of pulses that collectively form a meter, acting as a super-structure emerging from the lower-level pulse structure. Similarly, timbres can blend to form emergent textural patterns, a phenomenon well-known in orchestration. Thus, in the domains of rhythm and timbre, super-structures emerge from underlying structures and processing mechanisms.</p>
<p>However, there is a clear distinction between music and speech in terms of the degree and depth of pattern emergence. Blending timbres is less common in speech, as it may blur the signal and hinder semantic understanding. Moreover, in music, performers often co-regulate their actions to generate joint pattern emergence at rhythmic, pitch, and timbre levels. This joint emergence is far less prominent in speech; for instance, in heated debates, a moderator may intervene to ensure only one speaker talks at a time.</p>
<p>These examples illustrate that music, compared to speech, has an extraordinary capacity for pattern emergence. This capacity is a strong asset for (self-)augmentation, as it allows emergence to be achieved, when it might otherwise be non-existent. A striking example is Sardinian throat singing (<em>cantu a tenore</em>), where self-augmentation is physically perceived in the quality of the blending. This style is typically performed by groups of four male singers standing in a close circle.</p>
</div>
<div id="endowed-expression" class="section level3 unnumbered">
<h3>Endowed expression<a class="anchor" aria-label="anchor" href="#endowed-expression"><i class="fas fa-link"></i></a>
</h3>
<p>Emergent patterns do not exist in isolation. When produced by humans, these patterns become <em>endowed with expression</em>, referring to the gestures involved in both encoding and decoding sounds<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See Godøy and Leman, 2010.&lt;/p&gt;"><sup>13</sup></a>. Gestures define movements as expressive, and during the encoding of sounds, gestural traces become an integral part of the sound pattern and its emergent features. For instance, during singing, musical pitch exhibits portamento and intonation, and intervals between certain pitches may be shortened or lengthened to articulate rhythms.</p>
<p>In essence, gestural traces in sound define articulations (e.g., legato and staccato), sound color, musical narrative, and dynamics (e.g., crescendo and diminuendo), either complementing or integrating with emergent patterns. Consequently, human gestures become embedded in the musical structure, creating emergent patterns endowed with gestural expression.</p>
<p>We argue that this endowment of emergent patterns with expression provides another incentive for creating self-augmented interaction states. Emergent patterns endowed with expression facilitate augmentation. Emergence itself is a form of augmentation, and expression enhances what would otherwise be perceived as mere bodily movement.</p>
</div>
</div>
<div id="interaction-capacities" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Interaction capacities<a class="anchor" aria-label="anchor" href="#interaction-capacities"><i class="fas fa-link"></i></a>
</h2>
<p>At this point, I recognize that the theory of self-augmented interaction remains somewhat vague and undefined. However, both the theories of environmental interaction and the nature of music (as emergent patterns endowed with expression) point towards Gestalt theory. This theory posits that when parts are combined, the result is more than the sum of its parts. Essentially, our theory is a reinterpretation of Gestalt principles through the lens of modern cognitive science.</p>
<p>To complete our set of concepts introduced earlier in this chapter, we add three additional concepts: affordance, entrainment, and narration. A brief mention of these concepts will suffice.</p>
<div id="affordance" class="section level3 unnumbered">
<h3>Affordance<a class="anchor" aria-label="anchor" href="#affordance"><i class="fas fa-link"></i></a>
</h3>
<p>An <em>affordance</em> is a property of music (as a sonic form), and the ability to act upon an affordance offered by the music is called affordance capacity. Therefore, music possesses affordance capacity; it guides action. Affordances have sometimes been linked with the notion of frozen emotion, where composers and performers encode emotions in music, and listeners have the capacity to decode these emotions. As affordances, these frozen emotions are activated through music. In fact, it is possible to replace “emotion” with “gesture,” and the idea still holds: sound traces set gestures into action. Good music, I believe, possesses a strong affordance capacity.</p>
</div>
<div id="entrainment" class="section level3 unnumbered">
<h3>Entrainment<a class="anchor" aria-label="anchor" href="#entrainment"><i class="fas fa-link"></i></a>
</h3>
<p><em>Entrainment</em> is the capacity to adapt and align with music, either continuously, where movement flows with the music, or discretely, where movement marks musical events<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For introductions to entrainment, see f.i. Clayton et al. (2005) and Trost et al. (2017). &lt;/p&gt;"><sup>14</sup></a>. As the term suggests, entrainment implies that something in the music attracts the listener, acting as a driver for human activity. This could be motion, expression, or emotion.</p>
<p>In the context of this book, entrainment will be specifically defined as (co-regulated) sensorimotor synchronization. Notably, entrainment has been linked to a subliminal bias that reduces prediction errors in aligning body movement with sound cues<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Repp and Su (2013) is a classical reference.&lt;/p&gt;"><sup>15</sup></a>. Similar to the tempo maintained by a musical ensemble, entrainment can be understood as either an error-correction mechanism or a dynamic adaptation system.</p>
<p>In the final chapters (see <a href="chapTappers1.html#chapTappers1">7</a> and <a href="chapTappers2.html#chapTappers2">8</a>) we will demonstrate that entrainment, in the context of two individuals interacting with each other, involves both error-correction and dynamic adaptation. This process engages a contrasting dynamic between intentionally acted events and perceived events. Our goal is to operationalize this concept. However, it is important to note that entrainment can also be defined more broadly as the capacity to respond to cues or as a brain principle acting on neuronal oscillations.</p>
</div>
<div id="narration" class="section level3 unnumbered">
<h3>Narration<a class="anchor" aria-label="anchor" href="#narration"><i class="fas fa-link"></i></a>
</h3>
<p>Last but not least, we must mention <em>narration</em>, one of the least understood yet most important principles in music research. Narration refers to storytelling and is often associated with the human encoder. For instance, jazz musicians use patterns and previously trained playing gestures as a kind of alphabet to construct larger arcs of phrases, binding them into larger structures—stories in music. The manner of telling a narrative, i.e., the performance, is equally important, as it builds anticipation, entrainment, and affordance capacity.</p>
<p>A prime example is Clifford Brown, a legendary jazz trumpeter and composer who tragically died at the age of 25 in a car crash. Brown was renowned for his ability to tell stories through his music, using patterns and trained playing gestures to construct larger arcs of phrases. One of his most famous improvisations is on <em>Joy Spring</em>. In several recorded solos of this piece, similar phrase components are arranged differently. One of these solos is widely regarded as one of the best solo improvisations ever played. Brown’s work in jazz was as striking for its architectonic structure as for its emotional immediacy.</p>
<p>Unfortunately, narration is not often integrated in musicological studies on embodiment, prediction, and expression. We have a small contribution in chapter <a href="chapDancer.html#chapDancer">4</a> but it’s limited and more work is needed in this domain where sensorimotor and cognitive capacities intersect.</p>
<p>In short, music involves several capacities that are necessary to interact with the environment. These capacities are well-suited to process the dynamic structure of music, and in return, the emergent patterns.</p>
</div>
</div>
<div id="music-addiction" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Music addiction<a class="anchor" aria-label="anchor" href="#music-addiction"><i class="fas fa-link"></i></a>
</h2>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chapTheoryInteractionHypothesis"></span>
<img src="Figures/chapTheory_InteractionHypothesis.png" alt="Engine for self-augmented interaction" width="57%"><p class="caption">
Figure 1.1: Engine for self-augmented interaction
</p>
</div>
<p>Let us return to the idea, often espoused by grandmothers, that physical effort and endurance are necessary for self-improvement. Why would people engage in interactions that demand physical effort? The likely answer is that it pays off, and a more compelling explanation may lie in the biological processes that govern reward and pleasure. I prefer to view this as a form of pro-social addiction<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See Robbins et al. (2010) for theories of addiction.&lt;/p&gt;"><sup>16</sup></a>.</p>
<p>Figure <a href="chapTheory.html#fig:chapTheoryInteractionHypothesis">1.1</a> offers a rough model suggesting that interaction with music engages physical effort, expression, and prediction mechanisms. Together they co-engage arousal, valence, and agency, which trigger reward-related processes, based on dopamine-spread in the brain, that drive human subjects to engage more with music. As in addiction.<br>
Such a cycle may support the realization of self-augmented interaction states. Such states are rewarding and pleasurable, and it probably sets the scene for more such states. As in pro-social addiction.</p>
<p>We propose that <em>emergent patterns endowed with expression</em> facilitate the generation of self-augmented interaction states. These patterns align with the human capacity to capture affordances and engage in entrainment. The co-regulation of actions among multiple individuals creates a social context that can be motivating, and the formation of a musical narrative can be compelling. Moreover, this process can be rewarding in numerous ways.</p>
<p>The theory thus posits that music, due to the human disposition, offers and facilitates opportunities for self-augmented interaction states. As previously mentioned, this book aims to investigate specific elements of this broader theoretical claim, with a primary focus on statistical modeling.</p>
</div>
<div id="note-about-expression-theory" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Note about expression theory<a class="anchor" aria-label="anchor" href="#note-about-expression-theory"><i class="fas fa-link"></i></a>
</h2>
<p>To close this chapter, something should be said about the <em>expression theory</em> because it is often, like the narrative, and often neglected if not misunderstood.</p>
<p>Briefly stated, expression theory is based on the idea that expression in person A calls for an expressive response in person B, which in turn serves as stimulus for expression in A and so on, thus leading to a mutual exchange of expressions that might result in a particular interaction state.
This is indeed what is suggested in figure <a href="chapModelling.html#fig:chapTheoryBayesianModel2">2.2</a> (see next chapter), from a dynamic perspective.</p>
<p>Although this idea is very simple, some scholars find it hard to understand that expressions do not always require reasons for expressing. In many contexts, expressions really don’t require the inference of a presumed cause.</p>
<p>Interactions are often based on a direct and spontaneous gesturing, implying responses to patterns using gestures. This kind of direct gestural responding goes fast, and it is based on alignment, mirroring, including counterpoint gesturing. Interpreting an expression in terms of its presumed cause, largely depends on context and type of interaction. The main point is that expressions, to be valid and significant, do not always have to point to some deep underlying reason, a reason that can be found by inference. Instead, expressive interacting may occur without assuming the causes of the expression.</p>
<p>This is why. Expression is biological and largely intuitive. Therefore, rather than inferencing the latent state of being (known as the theory of mind theory), it is more appropriate to speak about gestural responding (based on mirroring). The real power of expression exchange is its dynamic ability to build up and maintain self-augmented states. Like in Sardinian throat singing, the real power is in the ability to blend voices and create an implied harmony or ghost tone. But others may hear in it an expression of a divine being. Expression theory may thus be understood in terms of an exchange of expressive gestures as patterns (and their possible inferred underlying states cannot always excluded) that steer-up the interaction towards self-augmented states.</p>
<p>The question raises whether Bayesian inference applies to patterns or to causes.
Sorry, but Bayesian inference is explained in the next chapter.
Anyhow, the answer is that it applies to both. Responding to an expression implies the processing of patterns and the assumption of a particular shape of the observed pattern can be seen as the prior of a Bayesian inference about that pattern. Inferring the cause of an expression would also apply that Bayesian inference scheme. In that case, the prior has a focus on the cause, for example, an emotion, or a character associated.
We’ll show that Bayesian inference is a general machinery for dealing with assumptions and observations, regardless whether it applies to expressive forms or expressive causes of those forms.</p>
</div>
<div id="conclusion" class="section level2" number="1.7">
<h2>
<span class="header-section-number">1.7</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>With a theory of music interaction in our hands, we have a perspective for understanding what music does with people and what people do with music, and why they do this. This theory evolved over several decades of research in musicology. However, it’s far from being an established theory because it needs refinement, or even a reformulation. It is biased by certain trends in cognitive science, and in the future, it is likely that neuroscience and neuro-biology will have a important contribution allowing for a refinement of the theory.</p>
<p>In this book, we use this theory as a general framework for case studies that highlight particular phenomena related to timing. We believe that an proper description and causal modelling of timing may contribute to our understanding of that same theory.
And yes, timing covers only a tiny small aspect of the entire theoretical framework but we believe that timing is an essential cornerstone. To be linked up with neuro-something in future research!</p>
<!-- ## References -->
<!-- Reybrouck and Van Dyck (2024), -->
<!-- Leman (2007, 2016), -->
<!-- Bader (2018), -->
<!-- Bonicco-Donato (2016), -->
<!-- Langner (2015), -->
<!-- Schneider (2018a, b), -->
<!-- Collins et al. (2014), -->
<!-- Sears et al. (2018), -->
<!-- Godøy and Leman (2010), -->
<!-- Clayton et al. (2005), -->
<!-- Trost et al. (2017), -->
<!-- Robbins et al. (2010) -->

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <h2>Provide your feedback</h2>
    <!--<p>Now is a great time to provide feedback</p>-->
        <ul class="list-unstyled">
<!--<li><a href="https://forms.gle/nq9RmbxJyZXQgc948">Provide feedback (5 min)</a></li>--><li><a href="https://www.ugent.be/lw/kunstwetenschappen/ipem/en/services/asil">art and science interaction lab</a></li>
        </ul>
<hr>
<nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapTheory"><span class="header-section-number">1</span> Theory</a></li>
<li><a class="nav-link" href="#self-augmented-interactions"><span class="header-section-number">1.1</span> Self-augmented interactions</a></li>
<li>
<a class="nav-link" href="#supporting-theories"><span class="header-section-number">1.2</span> Supporting theories</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#embodiment">Embodiment</a></li>
<li><a class="nav-link" href="#prediction">Prediction</a></li>
<li><a class="nav-link" href="#expression">Expression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#music-as-moving-sonic-forms"><span class="header-section-number">1.3</span> Music as moving sonic forms</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pattern-emergence">Pattern emergence</a></li>
<li><a class="nav-link" href="#endowed-expression">Endowed expression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#interaction-capacities"><span class="header-section-number">1.4</span> Interaction capacities</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#affordance">Affordance</a></li>
<li><a class="nav-link" href="#entrainment">Entrainment</a></li>
<li><a class="nav-link" href="#narration">Narration</a></li>
</ul>
</li>
<li><a class="nav-link" href="#music-addiction"><span class="header-section-number">1.5</span> Music addiction</a></li>
<li><a class="nav-link" href="#note-about-expression-theory"><span class="header-section-number">1.6</span> Note about expression theory</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">1.7</span> Conclusion</a></li>
</ul>
      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mlIpem/ExploringMusicInteractionWithR/blob/main/02_chapTheory.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mlIpem/ExploringMusicInteractionWithR/edit/main/02_chapTheory.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Exploring music interactions</strong>" was written by Marc Leman. It was last built on 2024-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
