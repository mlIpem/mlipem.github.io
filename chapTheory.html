<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Theory | render book</title>
<meta name="author" content="ML">
<meta name="description" content="All the assumptions – or call them hypotheses – that will tested in this book refer to a theory of music interaction. And this theory resembles what grandmother told us1, namely that, to achieve...">
<meta name="generator" content="bookdown 0.39 with bs4_book()">
<meta property="og:title" content="Chapter 1 Theory | render book">
<meta property="og:type" content="book">
<meta property="og:url" content="https://www.ugent.be/chapTheory.html">
<meta property="og:image" content="https://www.ugent.be/images/cover.png">
<meta property="og:description" content="All the assumptions – or call them hypotheses – that will tested in this book refer to a theory of music interaction. And this theory resembles what grandmother told us1, namely that, to achieve...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Theory | render book">
<meta name="twitter:description" content="All the assumptions – or call them hypotheses – that will tested in this book refer to a theory of music interaction. And this theory resembles what grandmother told us1, namely that, to achieve...">
<meta name="twitter:image" content="https://www.ugent.be/images/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Lato-0.4.9/font.css" rel="stylesheet">
<link href="libs/Roboto_Mono-0.4.9/font.css" rel="stylesheet">
<link href="libs/Montserrat-0.4.9/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.7.0/transition.js"></script><script src="libs/bs3compat-0.7.0/tabs.js"></script><script src="libs/bs3compat-0.7.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-99618359-1', 'auto');
      ga('send', 'pageview');

    </script><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-VDC2S0ZNH5"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-VDC2S0ZNH5');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">render book</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="active" href="chapTheory.html"><span class="header-section-number">1</span> Theory</a></li>
<li><a class="" href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></li>
<li><a class="" href="chapListener.html"><span class="header-section-number">3</span> Listener</a></li>
<li><a class="" href="chapDancer.html"><span class="header-section-number">4</span> Dancer</a></li>
<li><a class="" href="chapViolinist.html"><span class="header-section-number">5</span> Violin player</a></li>
<li><a class="" href="chapExoskeletons.html"><span class="header-section-number">6</span> Two violin players</a></li>
<li><a class="" href="chapTappers1.html"><span class="header-section-number">7</span> Two finger tappers – dynamic system</a></li>
<li><a class="" href="chapTappers2.html"><span class="header-section-number">8</span> Two finger tappers</a></li>
<li><a class="" href="chapConclusion.html"><span class="header-section-number">9</span> Conclusion</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.ugent.be/mleman/ExploringMusicInteractionsWithR">View book source <i class="fab fa-gitlab"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapTheory" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Theory<a class="anchor" aria-label="anchor" href="#chapTheory"><i class="fas fa-link"></i></a>
</h1>
<p>All the assumptions – or call them hypotheses – that will tested in this book refer to a theory of music interaction. And this theory resembles what grandmother told us<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Admitted, not only grandmother. Great writers and philosophers, such as Alghieri Dante in his &lt;em&gt;Divina Comedia&lt;/em&gt;, or Baruch Spinoza in his &lt;em&gt;Ethics&lt;/em&gt;, advocated the idea that effort is needed to achieve something, for example, according to Spinoza: to acquire knowledge and come closer to God, that is, closer to Nature.&lt;/p&gt;"><sup>1</sup></a>, namely that, to achieve something in life, a little effort is needed. The idea can be applied to human interaction with music. An effort in the form of an alert and active participation is needed to establish and interaction, and achieve something from that interaction, such as a feeling of power and beauty.</p>
<p>This chapter is an attempt to reformulate grandmother’s advice from the perspective of core theoretical concepts that are prominent in modern musicology.</p>
<div id="self-augmentated-interactions" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Self-augmentated interactions<a class="anchor" aria-label="anchor" href="#self-augmentated-interactions"><i class="fas fa-link"></i></a>
</h2>
<p>Basically, in this book we assume that people have an urge to create and engage themselves in particular interaction states involving music, because they experience benefits from doing it. These interaction states are special in the sense that they facilitate an increased alertness and attention, reduce stress, optimize perception and critical spatial-temporal actions, engaging a chain of bodily-related changes that affect human well-being. The states that facilitate these effects are called <em>self-augmented interaction states</em>.
They are <em>self</em>-augmented because people create these interactions states for themselves, volitional, by choice and decision.</p>
<p>Our goal is to find evidence that supports the theory (or falsify it).
However, at this moment, we are far from being able to prove this theory of <em>self-augmented interaction</em>, the reason being that the research domain is young and insufficiently mature to handle the enormous complexity of what is going on. Evidence is likely to be based on a joint effort of behavioral and neuroscience studies, involving neurobiology as a necessary component of the entire picture. Increased alertness, reduced stress, pleasure and motivation suggest the involvement of neurotransmitters and hormones such as norepinephrine, oxytocine, cortisol, dopamine that regulate brain functions, affecting behavioral activities during interactions<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For a review of neurochemical responses to music, see Reybrouck and Van Dyck (2024).&lt;/p&gt;"><sup>2</sup></a>.
We envision the complexity of a dynamic mechanism whose state can be build up over time, be maintained for a while, until it declines and becomes a normal state back again. This kind of complexity is challenging. Being able to identify the <em>self-augmented interaction state</em> is already a challenge.</p>
<p>We contribute to this theory by focusing on timing. It’s a tiny little part of the grand dynamic mechanism but we believe that timing is interesting because it is a distinguishing, measurable, and partly controllable, feature of interacting.</p>
<p>The paradigmatic example is the tempo of the music played by an ensemble of musicians. The tempo is a feature of timing and it is created and maintained by the collaborative effort and co-regulation of the musicians in the ensemble. It is a precarious state that is associated with the ensemble’s musical quality. For sure, tempo is not the only interesting feature of timing, but it’s an important one. It can be measured, for example, by identifying onsets in audio, and calculating the underlying common duration structure between two such intervals. When timing is stable, we have a special interaction state built upon alertness, physical effort, and fine sensory-motor control. These are expected features of a self-augmented interaction state. Other bodily features associated with those states can then be studied in more detail.</p>
<p>In short, the theory of interaction is about the urge to improve ourselves, and to invest some effort in order to achieve this improvement. Why? Grandmother’s advice is rooted in the human biology of pleasure and reward. It is based on an injection of effort in order to get something rewarding out of it.
The study of <em>self-augmented interaction</em> is not within our reach, yet our goal is to contribute to it, firstly by posing it as a valuable theory, secondly, by offering data-analysis tools that sharpen some details of contributing insights that may lead us towards the ultimate theory.</p>
</div>
<div id="supporting-theories" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Supporting theories<a class="anchor" aria-label="anchor" href="#supporting-theories"><i class="fas fa-link"></i></a>
</h2>
<p>The ultimate theory, which is a complex dynamic theory, draws upon a number of other theories about humans interacting with their environment.
These theories relate to embodiment, prediction and expression.</p>
<div id="embodiment" class="section level3 unnumbered">
<h3>Embodiment<a class="anchor" aria-label="anchor" href="#embodiment"><i class="fas fa-link"></i></a>
</h3>
<p><em>Embodiment</em> holds the idea that humans have a repertoire of gestures and actions, which they use for coding and decoding music<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;see Leman(2007, 2016)&lt;/p&gt;"><sup>3</sup></a>. During music playing (or coding), gestures shape phrasing, articulations, and intonations, and musical structures will therefore reflect traces of these gestures in the <em>moving sonic forms</em> that define music. During music listening and dancing (or decoding), the human decoder recruits bodily gestures from the own gesture repertoire to align with the gestural traces in the moving sonic forms. The gestures allow the human decoder to develop an understanding of music from a gestural/action perspective, that is, by embodying the gestural traces in the music.
Musical embodiment as a encoding and decoding of gestures works for many people because gestures and actions have a shared repertoire of gestures and actions. That repertoire is partly biological, due to the shared bio-mechanics and biological basis of all humans over all cultures, and partly cultural, due to codifications of how people move and behave in different cultural contexts.</p>
<p>Timing is rooted in embodiment because in music it is based on bodily actions. Hence, in studying timing in view of self-augmented interaction states, we can strongly draw upon gestures and movements as an alternative to audio. For example, in violin playing it is hard to detect onsets and there may be ways to study timing from bowing gestures.</p>
<!-- The embodiment theory also assumes entrainment mechanisms that allow for *sensorimotor synchronization* (e.g. tapping along with the beat) and *attuning based on action-perception coupling*. The latter is understood as the ability to (bodily) move along with moving sonic forms, such as moving the arm along with the melody. The embodiment theory copes with the idea that gesturing can be connected with the emotional system, thus offering a genuine basis for *empathic gesturing*.  -->
<!-- More about this theory is found in Leman (2007). -->
<!-- Traces in music have an *affordance* capacity, meaning that they offer cues for human gestural responding. -->
<!-- Accordingly, a *mirroring* can possibly be established between encoding gestures and decoding gestures.  -->
<!-- The strong mirroring hypothesis says that listeners/dancers can mirror performer’s gestures. The weak mirroring hypothesis says that listener/dancers can mirror gestural aspects of the moving sonic forms. The latter accounts for the fact that musical instruments may mediate human gesturing, thus making it less evident to establish a correlation between movements of the performer and movements of the listener/dancer.  -->
</div>
<div id="prediciton" class="section level3 unnumbered">
<h3>Prediciton<a class="anchor" aria-label="anchor" href="#prediciton"><i class="fas fa-link"></i></a>
</h3>
<p>Embodiment is connected with <em>prediction</em>, or the idea that moving sonic forms can be anticipated by the human encoder/decoder<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For an introductory article, see Seth (2015).&lt;/p&gt;"><sup>4</sup></a>. Often this anticipation is based on gesturing. For example, during dancing on the beat of music, gestural alignment and body movement is typically initiated ahead of the beat. If it were not ahead, the movement would be too late. Moving in synchrony with the musical beat thus proves that the embodiment is fundamentally predictive. It leads to very strong effects as in the <em>reverse causality</em> illusion<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;David Hume, in his &lt;em&gt;A Treatise of Human Nature&lt;/em&gt;, launched the idea that causality in the brain is a matter of association, taking into account time and a contiguous relationship between cause and effect.&lt;/p&gt;"><sup>5</sup></a>. In this illusion, it appears as if gesturing <em>causes</em> the music because there is correlation of gesture and music, and the gesture comes before musical events that mark its timing, due to the prediction engine that drives the gesturing. Obviously, the perception of causality is illusory because the music goes on when the decoder’s gesturing stops. Nevertheless, the experience of reverse causality illusion may create a strong feeling of control (agency) in the human decoder. Establishing reverse causality through self-created augmented interaction states may be crucial ability of the human decoder to create empowering effects.</p>
<p>Due to prediction, or anticipation, it is possible to co-regulate our own action with those heard from another musician, in such a way that a stable tempo can be established. Prediction is therefore a crucial element for understanding how self-augmented interaction states can be maintained. Effects such as the <em>reverse causality</em> illusion can be a direct consequence of prediction.</p>
</div>
<div id="expression" class="section level3 unnumbered">
<h3>Expression<a class="anchor" aria-label="anchor" href="#expression"><i class="fas fa-link"></i></a>
</h3>
<p>Embodiment and prediction, when considered from the viewpoint of a social interaction, supports <em>expression</em>. This viewpoint holds that gestures are bio-social and coded-social signals and that the sender of such a signal is expressive when its receiver responds with a signal<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The viewpoint adopted here is based on scholars working on human interaction, such as David Hume, Charles Darwin, Adam Smith, Erving Goffman. See Bonicco-Donato (2016) for a historical overview.&lt;/p&gt;"><sup>6</sup></a>. By exchanging gestures, sender and receiver typically tend to triggers
the embodied predictions through arousal, a state of attention, focus and emotional engagement that strongly contributes to self-augmented interaction states. It could lead to ritualized behavior when the exchanging gestures are habituated.</p>
<p>Expression is crucial when people interact with each other and we speak about <em>expressive timing</em> when timing works as a signal evoking responses from the audience.
By mutual exchange of gestural expressions, and thanks to predictions, humans are capable of interacting with each other in a coordinated manner. As musical rhythms enhance these exchanges of bodily expression through rhythms, it will facilitate the formation of a co-regulated interaction and pave the way for self-augmentation.</p>
<p>Together the three constructs offer a way for understanding the musical self-augmentation in terms of <em>gestural anticipatory expressions</em>. Ultimately, its dynamic results in a state of being where senses, cognitive abilities, and emotional engagement have become sharpened and function optimally.</p>
</div>
</div>
<div id="moving-sonic-forms" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Moving sonic forms<a class="anchor" aria-label="anchor" href="#moving-sonic-forms"><i class="fas fa-link"></i></a>
</h2>
<p>Given the above theories, it is useful to define music from the perspective of <em>moving sonic forms</em> and its <em>emergent patterns endowed with expression</em>.
This characterization allows us to easily distinguish music from speech.</p>
<div id="pattern-emergence" class="section level3 unnumbered">
<h3>Pattern emergence<a class="anchor" aria-label="anchor" href="#pattern-emergence"><i class="fas fa-link"></i></a>
</h3>
<p><em>Pattern emergence</em> is the idea that a pattern has structural features conjugate with the human disposition for emergence. Due to the disposition and the structure, emergence may happen. For example, the auditory system is a disposition that transforms a harmonic structure composed of 600, 800, 1000, 1200 Hz in a pitch heard at 200 Hz<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For a review of the human auditory system and pitch as pattern emergence, see (Langner, 2015).&lt;/p&gt;"><sup>7</sup></a>.
The auditory disposition may also fuse multiple of those harmonic patterns into a chord. And when such harmonic patterns are played in sequence, their fusion may elicit expectations, leading to tonal tensions and relaxation dynamics<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For reviews of pitch and timbre perception in music, see Schneider (2018a, b).&lt;/p&gt;"><sup>8</sup></a>. The bottom-up mechanisms of pattern emergence may compete with top-down mechanisms of patterns formed by habits, and this competition may influence the perception of tonal tension and relaxation, although the precise contributions of sensory (bottom up) and cognitive (long term memory) processing are still debated<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Collins et al. (2014) show how tonality can be understood from sensory processing.
Sears et al. (2018) for a discussion of sensory and cognitive tonality effects.&lt;/p&gt;"><sup>9</sup></a>.</p>
<p>A similar observation can be made for rhythms. Rhythms are made of pulses that subsume the meter as a super-structure that emerges from the lower-level pulse structure.
Similarly, timbres might blend and form emergent patterns of texture, a phenomenon that is well-known in orchestration.</p>
<p>Blending timbres is less obvious for speech because it may just blur the signal, making it less apt for understanding its semantics. Moreover, in music it often happens that different performers co-regulate their actions in order to generate a joint pattern emergence at rhythmic, pitch and timbre levels. Again, this phenomenon of joint speech is far less prominent in speech. In heathed debates a moderator will intervene and tone down, leaving the word to only one speaker.</p>
</div>
<div id="endowed-expression" class="section level3 unnumbered">
<h3>Endowed expression<a class="anchor" aria-label="anchor" href="#endowed-expression"><i class="fas fa-link"></i></a>
</h3>
<p>Music is characterized by the fact that emergent patterns get <em>endowed with expression</em>. The latter refers to the importance of gesturing during encoding and decoding<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See Godøy and Leman, 2010.&lt;/p&gt;"><sup>10</sup></a>. As discussed, gestures form a constituent part of the sound pattern. Gestures will mould musical pitch with portamento and intonation. In a similar way, gestures will make intervals shorter and longer in order to pronounce rhythms. Gestures will define articulations (e.g., legato and staccato), sound color, musical narrative, and dynamics (e.g., crescendo and diminuendo). In short, human gestures get embedded in the structure of music, creating emergent patterns endowed with gestural expression.
Such emergent patterns define moving sonic forms, necessary for generating self-augmented interaction states.</p>
</div>
</div>
<div id="interaction-capacities" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Interaction capacities<a class="anchor" aria-label="anchor" href="#interaction-capacities"><i class="fas fa-link"></i></a>
</h2>
<p>We are now ready to have a look at music from the perspective of human interaction capacities.
Three additional concepts complete our set of concepts introduced in this chapter.
They are: affordance, entrainment, and narration.</p>
<div id="affordance" class="section level3 unnumbered">
<h3>Affordance<a class="anchor" aria-label="anchor" href="#affordance"><i class="fas fa-link"></i></a>
</h3>
<p>An <em>affordance</em> is a property of the music and the capacity to act upon an affordance is called a affordance capacity. Affordances have sometimes been linked with the notion of <em>frozen emotion</em> and the idea that composers and performers encode frozen emotion in music, while listeners have the capacity to decode these emotions because they work as affordances. In fact, it is possible to replace emotion with gesture and the idea still stands.</p>
</div>
<div id="entrainment" class="section level3 unnumbered">
<h3>Entrainment<a class="anchor" aria-label="anchor" href="#entrainment"><i class="fas fa-link"></i></a>
</h3>
<p><em>Entrainment</em> is the capacity of a human decoder to adapt to the music, and aligh with it, either in a continuous manner, when movement flows along with the music, or in a discrete manner, when movement marks musical events<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For introductions to entrainment, see f.i. Clayton et al. (2005) and Trost et al. (2017). &lt;/p&gt;"><sup>11</sup></a>. As the word suggests, entrainment implies that there is something in the music that attracts the listener. In the context of (co-regulated) sensorimotor synchronization, entrainment has been associated with a bias to subliminally reduce prediction errors in the alignment of body movement with sound cues. While entrainment is often defined in relation to synchronization, as the dynamic adaptation of sensorimotor behavior due to a coupling, entrainment may also be defined in a broader perspective, as the capacity of giving a response to cues, or even as a brain-principle acting on neuronal oscillations.</p>
</div>
<div id="narration" class="section level3 unnumbered">
<h3>Narration<a class="anchor" aria-label="anchor" href="#narration"><i class="fas fa-link"></i></a>
</h3>
<p><em>Narration</em> refers to story telling, and it is often associated with the human encoder. Jazz musicians, for example, use patterns and previously trained playing gestures as a kind of alphabet to construct larger arcs of phrases, and they bind phrases to larger structures: stories in music. The way of telling a narrative, i.e., the performance, is equally important, and it builds anticipation, entrainment, and affordance capacity. A good example is Clifford Brown. He was a legendary jazz trumpeter and composer who died at the young age of 25 in a car crash. He was known for his ability to tell stories through his music, using patterns and previously trained playing gestures to construct larger arcs of phrases. One of his most famous improvizations are on <em>Joy Spring</em>. In serveral recorded solos of this piece, one hears similar phrase components arranged differently. One of these solos is widely regarded as one of the best solo improvisations ever played. Brown’s work in jazz was as striking for its architectonic structure as for its emotional immediacy.</p>
<p>In short, music involves sensorimotor and cognitive processes that are necessary to interact with the environment. These capacities are well-suited to process the dynamic structure of music, and in return, the emergent patterns and gestures offer a rich environment where interaction sharpens sensorimotor and cognitive capacities. Some investment in interaction will pay off in the form of self-augmentation.</p>
</div>
</div>
<div id="music-addiction" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Music addiction<a class="anchor" aria-label="anchor" href="#music-addiction"><i class="fas fa-link"></i></a>
</h2>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chapTheoryInteractionHypothesis"></span>
<img src="Figures/chapTheory_InteractionHypothesis.png" alt="Engine for self-augmented interaction" width="100%"><p class="caption">
FIGURE 1.1: Engine for self-augmented interaction
</p>
</div>
<p>Let us come back to grandmothers idea, that effort is needed to achieve something. So why would people engage themselves in self-augmented interactions that require effort? The answer is that it pays off. However, the compelling force behind this is likely related to the biological processes of addiction<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See Robbins et al. (2010) for theories of addiction.&lt;/p&gt;"><sup>12</sup></a>.</p>
<p>Figure <a href="chapTheory.html#fig:chapTheoryInteractionHypothesis">1.1</a> offers a rough biological model suggesting that interaction with music engages physical effort, expression, and prediction mechanisms that co-engage arousal, valence, and agency, activating reward-related processes, such as dopamine-spread in the brain, that drive human subjects to engage more with music, as in addiction.<br>
Such a cycle may support the realization of self-augmented interaction states.</p>
<p>We assume that <em>emergent patterns endowed with expression</em> facilitate the generation of self-augmented interaction states because these patterns fit with the human capacity for affordance and entrainment. A multi-person co-regulation of actions sets a social context that can be motivating, and the formation of a musical narrative can become compelling.
And it all pays off, in many ways.</p>
</div>
<div id="note-about-expression-theory" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Note about expression theory<a class="anchor" aria-label="anchor" href="#note-about-expression-theory"><i class="fas fa-link"></i></a>
</h2>
<p>To close this chapter, something should be said about the <em>expression theory</em> because it is probably less well understood, or even neglected, in cognitive science.</p>
<p>Briefly stated, expression theory is based on the idea that expression in person A calls for an expressive response in person B, which in turn serves as stimulus for expression in A and so on, thus leading to a mutual exchange of expressions that might result in a particular interaction state.
This is indeed what is suggested in figure <a href="chapModelling.html#fig:chapTheoryBayesianModel2">2.2</a>, from a dynamic perspective.</p>
<p>Although this idea is very simple, expressions do not always require reasons. In many contexts, expressions really don’t require the inference of a presumed cause. Interactions are often based on a direct and spontaneous gesturing, implying responses to patterns. This direct responding goes fast, and it is based on alignment, mirroring, including counterpoint gesturing. Interpreting an expression in terms of their presumed cause, largely depends on context and type of interaction. The main point is that expressions do not always have to point to some deep underlying state that requires inference to capture it. Thus, expressive interacting may occur without assuming the causes of the expression.</p>
<p>Expression is biological and largely intuitive. Therefore, rather than inferencing the latent state of being (known as the theory of mind theory), it is more appropriate to speak about gestural responding (based on mirroring). The real power of expression exchange is its dynamic ability to build up and maintain self-augmented states. Expression theory may thus be understood in terms of an exchange of expressive gestures as patterns (and their possible inferred underlying states cannot always excluded) that steer-up the interaction towards self-augmented states.</p>
<p>Having clarified this, the question raises whether Bayesian inference applies to patterns or to causes.
The answer is that it applies to both. Responding to an expression implies the processing of patterns and the assumption of a particular shape of the observed pattern can be seen as the prior of a Bayesian inference about that pattern. Inferring the cause of an expression would also apply that Bayesian inference scheme. In that case, the prior has a focus on the cause, for example, an emotion, or a character.
In short, the Bayesian inference is a general machinery for dealing with assumptions and observations, regardless whether it applies to expressive forms or expressive causes of those forms.</p>
</div>
<div id="conclusion" class="section level2" number="1.7">
<h2>
<span class="header-section-number">1.7</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>With a theory of music interaction in our hands, we have a perspective for understanding what music does with people and what people do with music, and why they do this. This theory evolved over several decades of research in musicology. However, it’s far from being an established theory because it needs refinement, or even a reformulation. It is biased by certain trends in cognitive science, and in the future, it’s likely that neuroscience and neurobiology will have a important contribution allowing for a refinement of the theory.</p>
<p>In this book, we use this theory as a general framework for case studies that highlight certain aspects of this theory.
As mentioned, our focus is mainly on timing, prediction, and entrainment. And yes, timing covers only a tiny small aspect of the entire theoretical framework but we believe that timing is an essential cornerstone for understanding. We are not sure about the ultimate theory, but it motivates us to know more. If it is true, it would be a huge discovery, impacting our understanding of education, why works, or doesn’t work and much more.</p>
</div>
<div id="references" class="section level2" number="1.8">
<h2>
<span class="header-section-number">1.8</span> References<a class="anchor" aria-label="anchor" href="#references"><i class="fas fa-link"></i></a>
</h2>
<p>Reybrouck and Van Dyck (2024),
Leman (2007, 2016),
Bonicco-Donato (2016) ,
Langner (2015),
Schneider (2018a, b),
Collins et al. (2014),
Sears et al. (2018),
Godøy and Leman (2010),
Clayton et al. (2005),
Trost et al. (2017),
Robbins et al. (2010)</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="preface.html">Preface</a></div>
<div class="next"><a href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <h2>Provide your feedback</h2>
    <!--<p>Now is a great time to provide feedback</p>-->
        <ul class="list-unstyled">
<!--<li><a href="https://forms.gle/nq9RmbxJyZXQgc948">Provide feedback (5 min)</a></li>--><li><a href="https://www.ugent.be/lw/kunstwetenschappen/ipem/en/services/asil">art and science interaction lab</a></li>
        </ul>
<hr>
<nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapTheory"><span class="header-section-number">1</span> Theory</a></li>
<li><a class="nav-link" href="#self-augmentated-interactions"><span class="header-section-number">1.1</span> Self-augmentated interactions</a></li>
<li>
<a class="nav-link" href="#supporting-theories"><span class="header-section-number">1.2</span> Supporting theories</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#embodiment">Embodiment</a></li>
<li><a class="nav-link" href="#prediciton">Prediciton</a></li>
<li><a class="nav-link" href="#expression">Expression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#moving-sonic-forms"><span class="header-section-number">1.3</span> Moving sonic forms</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pattern-emergence">Pattern emergence</a></li>
<li><a class="nav-link" href="#endowed-expression">Endowed expression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#interaction-capacities"><span class="header-section-number">1.4</span> Interaction capacities</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#affordance">Affordance</a></li>
<li><a class="nav-link" href="#entrainment">Entrainment</a></li>
<li><a class="nav-link" href="#narration">Narration</a></li>
</ul>
</li>
<li><a class="nav-link" href="#music-addiction"><span class="header-section-number">1.5</span> Music addiction</a></li>
<li><a class="nav-link" href="#note-about-expression-theory"><span class="header-section-number">1.6</span> Note about expression theory</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">1.7</span> Conclusion</a></li>
<li><a class="nav-link" href="#references"><span class="header-section-number">1.8</span> References</a></li>
</ul>
      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.ugent.be/mleman/ExploringMusicInteractionsWithR/blob/main/02_chapTheory.Rmd">View source <i class="fab fa-gitlab"></i></a></li>
          <li><a id="book-edit" href="https://github.ugent.be/mleman/ExploringMusicInteractionsWithR/edit/main/02_chapTheory.Rmd">Edit this page <i class="fab fa-gitlab"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>render book</strong>" was written by ML. It was last built on 2024-01-06.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
