<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Listener appreciation | Exploring music interactions</title>
<meta name="author" content="Marc Leman">
<meta name="description" content="In this chapter17, we delve into the question of what music does to listeners. Ultimately, the goal is to develop a theory of how people reflect on what music does to them. The chapter proposes a...">
<meta name="generator" content="bookdown 0.41 with bs4_book()">
<meta property="og:title" content="Chapter 3 Listener appreciation | Exploring music interactions">
<meta property="og:type" content="book">
<meta property="og:url" content="https://www.ugent.be/chapListener.html">
<meta property="og:image" content="https://www.ugent.be/images/cover.jpeg">
<meta property="og:description" content="In this chapter17, we delve into the question of what music does to listeners. Ultimately, the goal is to develop a theory of how people reflect on what music does to them. The chapter proposes a...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Listener appreciation | Exploring music interactions">
<meta name="twitter:description" content="In this chapter17, we delve into the question of what music does to listeners. Ultimately, the goal is to develop a theory of how people reflect on what music does to them. The chapter proposes a...">
<meta name="twitter:image" content="https://www.ugent.be/images/cover.jpeg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Lato-0.4.9/font.css" rel="stylesheet">
<link href="libs/Roboto_Mono-0.4.9/font.css" rel="stylesheet">
<link href="libs/Montserrat-0.4.9/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.8.0/transition.js"></script><script src="libs/bs3compat-0.8.0/tabs.js"></script><script src="libs/bs3compat-0.8.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-99618359-1', 'auto');
      ga('send', 'pageview');

    </script><!-- Google tag (gtag.js) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-CDTTJLM7N4"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-CDTTJLM7N4');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">Exploring music interactions</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="chapTheory.html"><span class="header-section-number">1</span> Theory</a></li>
<li><a class="" href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></li>
<li><a class="active" href="chapListener.html"><span class="header-section-number">3</span> Listener appreciation</a></li>
<li><a class="" href="chapDancer.html"><span class="header-section-number">4</span> Dancer synchronization</a></li>
<li><a class="" href="chapViolinist.html"><span class="header-section-number">5</span> Violin player visual feedback</a></li>
<li><a class="" href="chapExoskeletons.html"><span class="header-section-number">6</span> Two violin players’ haptic feedback</a></li>
<li><a class="" href="chapTappers1.html"><span class="header-section-number">7</span> Two finger tappers’ entrainment (Part 1)</a></li>
<li><a class="" href="chapTappers2.html"><span class="header-section-number">8</span> Two finger tappers’ entrainment (Part 2)</a></li>
<li><a class="" href="chapConclusion.html"><span class="header-section-number">9</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mlIpem/ExploringMusicInteractionWithR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapListener" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Listener appreciation<a class="anchor" aria-label="anchor" href="#chapListener"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This chapter is dedicated to the late Ir. Henk Jacobs (1953–2024), who was my PhD student in musicology from 2020 to 2024. Henk applied his expertise in marketing to musicology, and this chapter is highly inspired by his insights and feedback.&lt;/p&gt;"><sup>17</sup></a>, we delve into the question of what music does to listeners. Ultimately, the goal is to develop a theory of how people reflect on what music does to them.</p>
<p>The chapter proposes a theory of music appreciation and we investigate how listeners rationalize about this appreciation, using a large empirical survey on music appreciation. In a way, the methodology is Bayesian because theory is used to develop a questionnaire for a survey. A statistical model is then used for analyzing the survey responses. That model draws upon the theory, and based on data, it informs the updated theory. A few such cycles lead to a refined theory.</p>
<p>In this chapter, we are not studying how listeners interact with music; rather, we study how listeners reflect on their interactions with music. Therefore, this chapter is more offline than online. It focuses on the listener, not the performer. Nevertheless, performers are also listeners. In that sense, this chapter sketches a global background for the chapters that follow.</p>
<p>This chapter depends on the following scripts for data preparation, plotting, modelling and model plotting:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapAll_00_Initialization.R"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapAll_01_Functions.R"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapListener/chapListener_02_DataPreparation.R"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapListener/chapListener_03_DataPlotting.R"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapListener/chapListener_04_Modelling.R"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="st">"Code/chapListener/chapListener_05_ModelPlotting.R"</span><span class="op">)</span></span></code></pre></div>
<div id="workflow" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Workflow<a class="anchor" aria-label="anchor" href="#workflow"><i class="fas fa-link"></i></a>
</h2>
<p>The chapter adopts a theory-driven approach as outlined in the previous chapter in figure <a href="chapModelling.html#fig:chapTheoryBayesianModel1">2.1</a>. The <em>subject</em> is the listener who interacts with music. However, in this chapter, we focus on the indicators.
We probe the listener with questions about appreciation and motivation, forcing the listener to reflect on what happened during listening. Using the questions, we aim at building a model of the listener’s appreciation.</p>
<p>To fully grasp the structure of this chapter, it is instructive to consider the workflow of figure <a href="chapListener.html#fig:chapListenerWorkflow">3.1</a>, which also applies this Bayesian epistemology.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chapListenerWorkflow"></span>
<img src="Figures/chapListener_Workflow.jpg" alt="Overview of theory and statistical modelling" width="100%"><p class="caption">
Figure 3.1: Overview of theory and statistical modelling
</p>
</div>
<p>The starting point is a theory of music appreciation.
Based on this theory, a questionnaire and a statistical model is generated.
The questionnaire is launched in a survey, and the answers to this questionnaire, the data, are processed and transmitted to the statistical model.
The statistical modelling then serves as feedback to the theory.</p>
<p>In short, the theory which we start from is the prior, the data given the theory is the likelihood, and the updated theory is the posterior, which in turn becomes the new theory.</p>
<p>In what follows, we hook our wagon to an acceptable stage of this iteration, and we’ll try to further refine the results obtained.</p>
</div>
<div id="theory" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Theory<a class="anchor" aria-label="anchor" href="#theory"><i class="fas fa-link"></i></a>
</h2>
<p>Our approach, as it is currently developed, draws from the fields of <em>neurobiology</em>, <em>marketing</em>, and <em>musicology.</em> These fields enable us to conceptualize music appreciation as the rating assigned by listeners to the gratification they experience while engaging with music.</p>
<ul>
<li><p><em>Neurobiology</em> suggests that gratification (reward) arises from the brain’s chemistry fabric, such as the neurotransmitter dopamine. It diffuses throughout the brain, eliciting a pleasurable sensation and triggering a desire for more —- a behavioral pattern akin to seeking out further stimuli that induce this reward, reminiscent of addiction.</p></li>
<li><p>In the realm of <em>marketing</em>, this pattern is called <em>wanting</em>. It is linked to the perceived value of a product, which reflects an overall appreciation indicative of customer satisfaction regarding the product’s functionalities and qualities. These qualities can also be assessed through a series of questionnaires designed to probe various aspects.</p></li>
<li><p>Finally, in the domain of <em>musicology</em>, it is proposed that music profoundly influences the listener’s experience and this can be analyzed in terms of sub-categories of that experience, such as immersion, embodiment, anticipation, emotion and expression. Some of these concepts have been discussed in chapter <a href="chapTheory.html#chapTheory">1</a>.</p></li>
</ul>
<p>Combining these insights from neurobiology, marketing, and musicology, a theory of music appreciation can be outlined as follows:
When a listener engages with music, a dynamic anticipation-reward-motivation loop is initiated, leading to experiences such as being emotionally moved, deeply absorbed, urge to move, or profoundly touched. The listener can subsequently reflect upon these experiences. The listener can also reflect upon the effect of these experiences, as they may culminate in a pleasurable bodily reward, which is positively evaluated.
Upon reflection, the listener may also identify specific qualities in the music which are assumed to have contributed to these pleasurable experiences.
Finally, the appreciation can be expressed by means of a global score, say from 1 to 10.</p>
<p>Obviously, any comprehensive theory of music appreciation must also consider the influence of context. Factors such as the setting in which the music is heard (e.g., live concert versus radio broadcast), the listener’s mood (happy, sad), demographic background (young, old), and other environmental variables can significantly impact how listeners perceive and interpret their musical experiences. Therefore, when conducting surveys or assessments, it is essential to account for these contextual factors to ensure clarity and accuracy in participants’ responses, thus avoiding potential confusion or misinterpretation.</p>
<p>However, we believe that appreciation is independent of the specific type of music being listened to. Consider for example sad music. Despite its melancholic nature, a piece of sad music can trigger intense embodied experiences that are highly valued by listeners, leading to a high appreciation score. Conversely, happy music may not always receive high appreciation, for example when it fails to engage the listener with a compelling rhythm or groove.
In addition, the appreciation is agnostic to factors such as gender or age.</p>
<p>Overall, in this theory, what truly matters is the activation of reward, pleasure, and wanting experiences by the music. Music appreciation reflects both an inward reflection, focusing on the nature and evaluation of the experience itself, and an outward reflection, directed towards the assumed qualities of the music. These reflections are influenced by various contextual factors such as background, gender, and setting.</p>
</div>
<div id="causal-model" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Causal model<a class="anchor" aria-label="anchor" href="#causal-model"><i class="fas fa-link"></i></a>
</h2>
<p>The appreciation theory can be clarified as a network of variables and relationships between variables. However, what is shown here is already the outcome of a considerable study involving fine-tuning of concepts and modelling. As mentioned, we hook our wagon to an acceptable stage of this iteration and this is how the theory is currently conceived.</p>
<div id="directed-acyclic-graph-dag" class="section level3 unnumbered">
<h3>Directed acyclic graph (DAG)<a class="anchor" aria-label="anchor" href="#directed-acyclic-graph-dag"><i class="fas fa-link"></i></a>
</h3>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chapListenerDAG1"></span>
<img src="Figures/chapListener_Dagitty1c.jpg" alt="Causal model of appreciation shown as a DAG" width="100%"><p class="caption">
Figure 3.2: Causal model of appreciation shown as a DAG
</p>
</div>
<p>The network can be shown as a DAG, or directed acyclic graph (DAG), as shown in figure <a href="chapListener.html#fig:chapListenerDAG1">3.2</a>. In this graph the theory is spelled out as a set of concepts and <em>causal</em> relationships among these concepts. Note that the use of the term <em>causal</em> has a very specific meaning in this context. A cause-effect relationship among two concepts means (i) that the information flow has a <em>direction</em>, in the sense that the cause precedes the effect in time, (ii) that there is an <em>association</em> (correlation) between the concepts, and (iii) that <em>no potential confounding variables</em> could account for the observed association. The DAG causality is tested by considering the logic of causal relations (see below). Note that we do not really model causal flow in the sense of what will be done in chapters <a href="chapTappers1.html#chapTappers1">7</a> and <a href="chapTappers2.html#chapTappers2">8</a>.</p>
<p>In this DAG, <code>Kind_of_Experience</code> stands for all kinds of experiences generated by the musical qualities (<code>Quality</code>) selected by the listener. Also other concepts are exposed to this one, with <code>Global_Appreciation</code> as outcome.
The concepts such as <code>Quality</code>, <code>Immersion</code> and so on, are called <em>latent variables</em> because they are not directly observable via questions. Instead, they live
underneath the surface of what is observable but we can probe them via questions.</p>
<p>The rationale is that musical qualities (<code>Quality</code>) affect the listener, causing experiences in the listener (<code>Kind_of_Experience</code>), including for instance, an increase in dopamine level generating pleasurable feelings and a wanting urge. <code>Quality</code> represents the musical attributes identified by the participant as contributing to the type of experience they had. It’s not the objective quality of music, but the subjective quality, as identified by each individual listener. <code>Quality</code> can be considered a determinant of <code>Kind_of_Experience</code>, which we also divide in sub-concepts <code>Immersion</code>, <code>Emotion</code>, and <code>Embodiment.</code>
These experiences set the scene for an assessment (<code>Evaluation</code>), which then causes the scoring for <code>Global_Appreciation</code>. The latter is the product value, an expression of the degree of wanting that product.</p>
<p>In the DAG, the ellipses marked with X are questions.
They have an incoming arrow because the answers are conceived as generated by the latent variables.
Pretty much like catching fish from different environments underneath the water surface. It’s the fish that bites.</p>
</div>
<div id="confounding-variables" class="section level3 unnumbered">
<h3>Confounding variables<a class="anchor" aria-label="anchor" href="#confounding-variables"><i class="fas fa-link"></i></a>
</h3>
<p>While the causal direction and the plausible associations show clear relationships among concepts, it might require careful logical reasoning in order to prevent confounding variables in such a network<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;see McElreath (2020) for a summary of those reasonings.&lt;/p&gt;"><sup>18</sup></a>.
Fortunately, a DAG can be tested using the tool <a href="https://dagitty.net/dags.html">Daggity</a>. It involves evaluating whether the assumed causal structure is consistent, that is, whether certain sets of variables are conditionally independent given other sets of variables, as predicted by the DAG. We are lucky, our DAG is safe. No adjustment is necessary to estimate the total effect of <code>Kind_of_experience</code> on <code>Global_Appreciation</code>, meaning that there are no confounding paths between the variable <code>Kind_of_experience</code> and the outcome variable <code>Global_Appreciation</code>.</p>
<p>We can test different interpretations of the model, for example, by drawing an arrow from <code>Kind_of_experience</code> to <code>Quality</code>, assuming that the <code>Kind_of_experience</code> is the cause of the quality recognized in the music.
However, one should be careful about possible open biasing paths, where confounding variable would bias the estimation of the causal effect between <code>Kind_of_experience</code> and <code>Global_Appreciation</code>.</p>
</div>
</div>
<div id="questionnaire" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Questionnaire<a class="anchor" aria-label="anchor" href="#questionnaire"><i class="fas fa-link"></i></a>
</h2>
<p>Based on the above theory for music appreciation, it is possible to define questions that allow us to measure the latent variables. The following table is a summary of all questions used in a survey (see below), except the open questions which we don’t handle here. In what follows, we briefly discuss how these questions are structured and processed.</p>
<div class="inline-table"><table class="table table-striped" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
(#tab:chapListener_datasetQ)Questionnaire with label, question summary and question. L5 means: Likert scale from 1 to 5
</caption>
<thead><tr>
<th style="text-align:left;">
label
</th>
<th style="text-align:left;">
question summary
</th>
<th style="text-align:left;">
question
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Title
</td>
<td style="text-align:left;">
Title Open
</td>
<td style="text-align:left;">
Give the title of the musical piece
</td>
</tr>
<tr>
<td style="text-align:left;">
Composer
</td>
<td style="text-align:left;">
Composer Open
</td>
<td style="text-align:left;">
Give the name of the composer
</td>
</tr>
<tr>
<td style="text-align:left;">
Q1
</td>
<td style="text-align:left;">
Global_appreciation 1 to 10
</td>
<td style="text-align:left;">
Give your global appreciation score
</td>
</tr>
<tr>
<td style="text-align:left;">
Q2
</td>
<td style="text-align:left;">
Connection L5
</td>
<td style="text-align:left;">
|With this piece of music I feel no connection
</td>
</tr>
<tr>
<td style="text-align:left;">
Q3
</td>
<td style="text-align:left;">
Arousal L5
</td>
<td style="text-align:left;">
|In this piece I hear density, energy, activity, excitement
</td>
</tr>
<tr>
<td style="text-align:left;">
Q4
</td>
<td style="text-align:left;">
Valence L5
</td>
<td style="text-align:left;">
|In this piece I hear joy, optimism, positive emotions
</td>
</tr>
<tr>
<td style="text-align:left;">
Q6
</td>
<td style="text-align:left;">
Loving Yes(1)/No(2)
</td>
<td style="text-align:left;">
|Does the piece of music have certain core qualities for you? These are characteristics that you immediately think of when you consider the piece of music. They give you a euphoric feeling, they touch you. They are also called ‘WOW’ effects. Because of these core qualities, you love the piece of music.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q8
</td>
<td style="text-align:left;">
Liking Yes/No
</td>
<td style="text-align:left;">
|Question 2: Besides core qualities, the piece of music may have distinctive elements for you that are certainly worth listening to. They give you a good feeling and often create a ‘click’ with the piece of music. Do you recognize these elements?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q10
</td>
<td style="text-align:left;">
Indifferent Yes/No
</td>
<td style="text-align:left;">
|Question 3: Does the piece of music have certain distinctive elements for you that you don’t care for? These may be found, for example, in transitional passages. They have no positive, but also no negative effect on your musical experience.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q12
</td>
<td style="text-align:left;">
Disliking Yes/No
</td>
<td style="text-align:left;">
|Question 4: Even if the piece is your first KLARA top 100 piece, it may still have certain distinctive elements that you don’t like. Do you recognize this?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q14
</td>
<td style="text-align:left;">
Disturbing Yes/No
</td>
<td style="text-align:left;">
|Question 5: Does the piece of music have certain distinctive elements that bother you or even annoy you?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q16
</td>
<td style="text-align:left;">
Missing Yes/No
</td>
<td style="text-align:left;">
|Question 6: It may be that the piece of music lacks certain elements that are important to you. Is this the case?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q18
</td>
<td style="text-align:left;">
Sphere L5
</td>
<td style="text-align:left;">
|Statement 1: The atmosphere I hear in the music influences my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q20
</td>
<td style="text-align:left;">
Structure L5
</td>
<td style="text-align:left;">
|Statement 2: The structure/composition of the piece of music influences my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q22
</td>
<td style="text-align:left;">
Melody L5
</td>
<td style="text-align:left;">
|Statement 3: The melody of this piece of music influences my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q24
</td>
<td style="text-align:left;">
Harmony L5
</td>
<td style="text-align:left;">
|Statement 4: The harmony/ensemble singing in this music influences my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q26
</td>
<td style="text-align:left;">
Time L5
</td>
<td style="text-align:left;">
|Statement 5: The rhythm and tempo of this piece of music influences my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q28
</td>
<td style="text-align:left;">
Texture Categorial
</td>
<td style="text-align:left;">
|Does the piece of music have vocals or is it instrumental?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q29
</td>
<td style="text-align:left;">
Texture L5
</td>
<td style="text-align:left;">
|The lyrics influence my choice.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q31
</td>
<td style="text-align:left;">
Attention L5
</td>
<td style="text-align:left;">
|Statement 1: Generally, this piece of music demands my full attention (for example, ‘forgetting time’, ‘full concentration on the music (and the lyrics)’, ‘completely shut off from the surroundings’).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q33
</td>
<td style="text-align:left;">
Absorbtion L5
</td>
<td style="text-align:left;">
|Statement 2: Generally, it feels as if this piece of music and I are one (for example, ‘being completely captivated by the music’, ‘feeling one with the music’, ‘feeling after listening as if I have been on a journey’).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q35
</td>
<td style="text-align:left;">
Engagement L5
</td>
<td style="text-align:left;">
|Statement 3: Generally, I feel completely engaged with this piece of music (for example, sympathy, empathy, and identification with the performer(s), composer, characters in the music, meaning of the music).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q37
</td>
<td style="text-align:left;">
Moving L5
</td>
<td style="text-align:left;">
|Statement 4: Generally, I move automatically with this piece of music (for example, tapping to the beat, swaying, conducting along).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q39
</td>
<td style="text-align:left;">
Participation L5
</td>
<td style="text-align:left;">
|Statement 5: Generally, I sing, whistle, or hum along with this piece of music.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q41
</td>
<td style="text-align:left;">
Physical L5
</td>
<td style="text-align:left;">
|Statement 6: Generally, I experience physical sensations with this piece of music (for example, goosebumps, shivers down the spine, lump in the throat, tears of emotion, heart beats faster).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q43
</td>
<td style="text-align:left;">
Emotions L5
</td>
<td style="text-align:left;">
|Statement 7: Generally, I experience certain emotions with this piece of music (for example, nostalgia, energy, happiness, sadness, inspiration).
</td>
</tr>
<tr>
<td style="text-align:left;">
Q45
</td>
<td style="text-align:left;">
Mood L5
</td>
<td style="text-align:left;">
|Statement 8: Generally, my mood changes when listening to this piece of music.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q47
</td>
<td style="text-align:left;">
Enjoying L5
</td>
<td style="text-align:left;">
|I can intensely enjoy this piece of music.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q48
</td>
<td style="text-align:left;">
Touching L5
</td>
<td style="text-align:left;">
|This piece of music affects me in a positive way.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q49
</td>
<td style="text-align:left;">
Annoying L5
</td>
<td style="text-align:left;">
|I can get extremely annoyed by this piece of music.
</td>
</tr>
<tr>
<td style="text-align:left;">
Q50
</td>
<td style="text-align:left;">
Listening Categorial
</td>
<td style="text-align:left;">
|How long do you typically listen to music in a day?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q51
</td>
<td style="text-align:left;">
Playing Categorial
</td>
<td style="text-align:left;">
|How long do you typically play music yourself in a day?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q52
</td>
<td style="text-align:left;">
Level Categorial
</td>
<td style="text-align:left;">
|How would you rate your musical ability?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q53
</td>
<td style="text-align:left;">
Age Categorial
</td>
<td style="text-align:left;">
|What is your age?
</td>
</tr>
<tr>
<td style="text-align:left;">
Q54
</td>
<td style="text-align:left;">
Gender Categorial
</td>
<td style="text-align:left;">
|You are
</td>
</tr>
</tbody>
</table></div>
<p>This is how the questions are linked up with the DAG. Note that the DAG’s questions are marked with <code>X</code> but that’s just a dummy. In what follows, we mark questions with <code>Q</code> as indicated in table @ref(tab:chapListener_datasetQ). Accordingly, to complete the DAG with the real questions, it is necessary to take the following steps into consideration:</p>
<ul>
<li><p><code>Global_appreciation</code> in the DAG is estimated with a single question (Q1), scored on a scale from 1 to 10. It reflects how eager the listener would want this music.</p></li>
<li><p><code>Quality</code> in the DAG is based on six yes/no questions (Q6, Q8, Q10, Q12, Q14, Q16 in the dataset) probing different aspects of the musical quality. <code>Quality</code> is a score based on a weighted combination of these binary questions<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This is based on the so-called Kano model, see &lt;a href="https://www.interaction-design.org/literature/article/the-kano-model-a-tool-to-prioritize-the-users-wants-and-desire" class="uri"&gt;https://www.interaction-design.org/literature/article/the-kano-model-a-tool-to-prioritize-the-users-wants-and-desire&lt;/a&gt;.&lt;/p&gt;'><sup>19</sup></a>. It gives a colored assessment about the musical features believed to have influenced the experiences.</p></li>
<li><p><code>Evaluation</code> is based on three questions (Q47, Q48, Q49) probing the personal value of the experience specified in <code>Kind_of_experience</code>.</p></li>
<li><p><code>Kind_of_experience</code> is subdivided in <code>Immersion</code> (Q31, Q33, Q35), <code>Embodiment</code> (Q37, Q41, Q43) and <code>Emotion</code> (Q43, Q45). These questions probe the kind of experience that was generated by the music.</p></li>
</ul>
<p>There are some additional questions about demography such as hours listening (Q50), hours playing (Q51), education (Q52), listener’s age (Q53), and gender (Q54).
There are also two questions about arousal (Q3) and valence (Q4) attributes of the music. These questions span a space that is further divided into four parts (see below).</p>
<p>By utilizing this questionnaire, researchers can effectively gather data to understand and analyze the components of music appreciation outlined in the theory.</p>
</div>
<div id="survey-and-data" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Survey and data<a class="anchor" aria-label="anchor" href="#survey-and-data"><i class="fas fa-link"></i></a>
</h2>
<p>Validation of the theory was based on a large survey.
This survey was conducted in collaboration with VRT-Klara, the classic music radio of the Flemish radio and television broadcasting company. In the edition of the Klara-Top100 in 2023, listeners could participate in our survey, using a redirect to a survey platform (Qualtrix) mounted at Ghent University. Approximately 1200 listeners responded to the survey and about 807 listeners completed the entire questionnaire.</p>
<p>The questionnaire was set up in two parts The first part, the listener had to focus on high appreciation, coded as <em>Liking</em> in Q57, while the second part, the listener had to focus on low appreciation, coded as <em>Disliking</em> in Q57.</p>
<p>This is how the data looks like:</p>
<p></p>
<pre><code>## 'data.frame':    1614 obs. of  38 variables:
##  $ Title                  : chr  "Concerto in D voor viool en orkest op35 3Finale" "Miserere" "Comptine dun autre t" "BWV 0248: Weihnachtsoratorium (Jauchzet, frohlocket! Auf, preiset die Tage)" ...
##  $ Composer               : chr  "Tsjaikovski, Pjotr Iljitsj" "Allegri, Gregorio" "Tiersen, Yann" "Bach, Johann Sebastian" ...
##  $ Q1                     : num  10 10 9 9 10 9 10 10 10 10 ...
##  $ Q2                     : num  1 1 1 1 1 1 2 1 1 1 ...
##  $ Q3                     : num  5 2 2 4 5 NA 5 2 1 2 ...
##  $ Q4                     : num  5 4 5 5 2 4 5 2 4 NA ...
##  $ Q6                     : num  1 1 1 1 0 0 1 1 1 1 ...
##  $ Q8                     : num  1 1 1 1 1 1 1 0 1 1 ...
##  $ Q10                    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Q12                    : num  0 1 0 0 0 0 0 1 0 0 ...
##  $ Q14                    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Q16                    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Q18                    : num  4 5 4 4 3 3 4 4 5 4 ...
##  $ Q20                    : num  4 4 3 4 3 3 4 2 5 4 ...
##  $ Q22                    : num  5 4 4 4 3 4 4 4 5 4 ...
##  $ Q24                    : num  5 5 3 4 3 4 4 2 5 4 ...
##  $ Q26                    : num  3 5 4 4 3 4 4 4 5 4 ...
##  $ Q28                    : num  1 3 1 2 1 1 1 2 1 1 ...
##  $ Q29                    : num  NA 3 NA 2 NA NA NA 4 NA NA ...
##  $ Q31                    : num  4 4 3 4 5 3 4 5 5 3 ...
##  $ Q33                    : num  4 4 4 3 4 4 4 4 5 3 ...
##  $ Q35                    : num  4 5 4 4 4 3 4 5 5 4 ...
##  $ Q37                    : num  5 4 2 3 4 4 3 3 4 4 ...
##  $ Q39                    : num  4 2 2 2 4 2 4 3 1 3 ...
##  $ Q41                    : num  3 5 2 3 3 3 3 5 5 4 ...
##  $ Q43                    : num  4 5 4 4 4 4 4 4 5 3 ...
##  $ Q45                    : num  5 5 3 3 4 3 4 3 5 4 ...
##  $ Q47                    : num  5 5 5 5 5 4 5 5 1 5 ...
##  $ Q48                    : num  5 5 5 5 5 4 5 5 1 5 ...
##  $ Q49                    : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Q50                    : Factor w/ 7 levels "0","0m-15m","15m-30m",..: 7 4 6 7 7 6 7 7 7 5 ...
##  $ Q51                    : Factor w/ 7 levels "0","0m-15m","15m-30m",..: 1 1 1 1 1 1 1 1 1 3 ...
##  $ Q52                    : num  0 0 0 0 0 0 0 0 0 3 ...
##  $ Q53                    : Factor w/ 10 levels "11-20","21-30",..: 6 4 7 6 6 7 7 6 6 7 ...
##  $ Q54                    : Factor w/ 3 levels "male","female",..: 2 2 2 2 1 1 2 2 2 2 ...
##  $ Q57                    : chr  "Liking" "Liking" "Liking" "Liking" ...
##  $ Quality                : num  2.5 1.33 2.5 2.5 1 ...
##  $ ArousalValence_category: chr  "high_pos" "low_pos" "low_pos" "high_pos" ...</code></pre>
<p></p>
<p>The 1614 observations come from the 807 subjects and each rated two pieces of music, one for <code>Liking</code> and one for <code>Disliking.</code>
Basically, this datset reflects the questionnaire, except that some labels have been constructed on top, such as Q57, Quality (based on Q6, Q8, Q10, Q12, Q14, Q16), and ArousalValence_category (based on Q3, Q4).</p>
</div>
<div id="inspect-the-data" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Inspect the data<a class="anchor" aria-label="anchor" href="#inspect-the-data"><i class="fas fa-link"></i></a>
</h2>
<p>Exploration of the data via plotting is always useful.</p>
<div id="appreciation" class="section level3 unnumbered">
<h3>Appreciation<a class="anchor" aria-label="anchor" href="#appreciation"><i class="fas fa-link"></i></a>
</h3>
<p>Here we show the global appreciation (labelled in the dataset as: Q1) on a scale from 1 to 10, per category <em>Liking</em> or <em>Disliking.</em>
The highest appreciated music gets a mean of 9.42, and standard deviation of 0.88.
The lowest appreciated music gets a mean of 2.89 and standard deviation of 1.66.
Apparently, the highest appreciated music is somewhat better defined than the lowest appreciated music which, in some case still gets an overall appreciation of 5/10 and in rare cases even 6/10.</p>
<div class="figure">
<span style="display:block;" id="fig:chapListeningPlot1"></span>
<img src="Figures/chapListener_Plot1.png" alt="Distribution of scorings for global appreciation (Q1) for music qualified as Liking and Disliking (Q57), with mean and standard deviation indicated" width="80%"><p class="caption">
Figure 3.3: Distribution of scorings for global appreciation (Q1) for music qualified as Liking and Disliking (Q57), with mean and standard deviation indicated
</p>
</div>
</div>
<div id="participants" class="section level3 unnumbered">
<h3>Participants<a class="anchor" aria-label="anchor" href="#participants"><i class="fas fa-link"></i></a>
</h3>
<p>A quick view on participants shows some striking facts. About 43% is 61-70 years old and only 17% is younger than 50 years old. There are about twice as many females compared to men. The majority has a low level of music education. Their main interaction with music is listening, not playing.</p>
<div class="figure">
<span style="display:block;" id="fig:chapListeningPlot2"></span>
<img src="Figures/chapListener_Plot2.png" alt="Some info about the listeners" width="100%"><p class="caption">
Figure 3.4: Some info about the listeners
</p>
</div>
</div>
<div id="affect-attribution" class="section level3 unnumbered">
<h3>Affect attribution<a class="anchor" aria-label="anchor" href="#affect-attribution"><i class="fas fa-link"></i></a>
</h3>
<p>The listeners’ attribution of arousal (Q3) and valence (Q4) was probed with a Likert scale from 1 to 5.
These scores can be interpreted as coordinates in a so-called circumplex model of affect<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The classical paper is by Russell (1980)&lt;/p&gt;"><sup>20</sup></a>, as shown in figure <a href="chapListener.html#fig:chapListenerArousalValenceLikeDislike">3.5</a>.
We use it here to identify only four different attributed affect categories of music.</p>
<ul>
<li><p>High-arousal high-valence is defined as <code>happy</code></p></li>
<li><p>high-arousal low-valence is defined as <code>aggressive</code></p></li>
<li><p>low-arousal high-valence is defined as <code>relaxing</code></p></li>
<li><p>low-arousal low-valence is defined as <code>sad</code>.</p></li>
</ul>
<p>The horizontal and vertical band in the middle show the neutral zone where listeners gave a score of 3 to either question.
The other scores reveal an interesting pattern.
Relaxing music is liked a lot, while aggressive music is mostly disliked.
Likewise, happy music is mostly liked, but sometimes disliked.
And sad music, it seems, is often disliked, but often also liked.</p>
<div class="figure">
<span style="display:block;" id="fig:chapListenerArousalValenceLikeDislike"></span>
<img src="Figures/chapListener_Plot4.png" alt="Liked and disliked music categorized by arousal and valence on a 5-point scale. To show the distributions in each point, we added jitter" width="100%"><p class="caption">
Figure 3.5: Liked and disliked music categorized by arousal and valence on a 5-point scale. To show the distributions in each point, we added jitter
</p>
</div>
<p>Overall, there seems to be quite some structure in the dataset.
Cronbach’s alpha gives a value of 0.84, which is considered good or excellent, meaning that the internal consistency of the dataset is high. The mean value of all correlations among all subjects is 0.54.
More can be said about the data but we restrict ourselves to the minimum needed for modelling.</p>
<div class="inline-table"><table class="table table-striped" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:chapListenerExplore2">Table 3.1: </span>Cronbach alpha and mean
</caption>
<thead><tr>
<th style="text-align:right;">
Cronbach.s.alpha
</th>
<th style="text-align:right;">
mean
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
0.8339282
</td>
<td style="text-align:right;">
0.5618432
</td>
</tr></tbody>
</table></div>
</div>
</div>
<div id="preparing-analysis" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> Preparing analysis<a class="anchor" aria-label="anchor" href="#preparing-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>The steps that follow can be seen as preparatory work for statistical modelling.
The rationale is that in view of the main question about the global appreciation (Q1), it is possible to
identify the questions that have low correlation with Q1.
Such questions will anyhow not really affect Q1 and therefore they can be deleted.</p>
<p>To start with, consider figure <a href="chapListener.html#fig:chapListenerExplore1a">3.6</a>.
The left panel shows the correlation among all (numerical) questions.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chapListenerExplore1a"></span>
<img src="Figures/chapListener_Cor1.png" alt="Correlation matrices. (left) original, (right) pruned" width="49%"><img src="Figures/chapListener_Cor2.png" alt="Correlation matrices. (left) original, (right) pruned" width="49%"><p class="caption">
Figure 3.6: Correlation matrices. (left) original, (right) pruned
</p>
</div>
<p>To check contributes to Q1, it may suffice to look at the first vertical column (with Q1 as label).
The important questions have high correlation values. Here we decided that
questions having a correlation value less then <span class="math inline">\(.5\)</span> can be deleted – or, as we did, set to zero.</p>
<p>The leftovers are shown in the right panel.
These are: Q2, Q31, Q33, Q35, Q37, Q39, Q41, Q43, Q47, Q48, Q49, and Quality.
Basically, questions that probe the structural features of the music do not correlate high, either because it is too difficult, and/or because listeners have a subjective focus on selected aspects, as captured by <code>Quality.</code> Also Q45 is left out, probably because listeners interpreted this question in different ways.</p>
<p>In short, based on a somewhat arbitrary correlation threshold, a pruned version of our questionnaire has been obtained.
The questions that pass the threshold can now be considered candidates for a model.
Let us see how far we get with the modelling.</p>
</div>
<div id="structural-equation-modelling" class="section level2" number="3.8">
<h2>
<span class="header-section-number">3.8</span> Structural equation modelling<a class="anchor" aria-label="anchor" href="#structural-equation-modelling"><i class="fas fa-link"></i></a>
</h2>
<p>A structural equation model (SEM) implements the DAG shown in figure <a href="chapListener.html#fig:chapListenerDAG1">3.2</a> as a statistical model. Meanwhile, however, we have identified the questions and therefore, from now on, we use the question labels as used in the table @ref(tab:chapListener_datasetQ).</p>
<p>The SEM estimates the strength and significance of the association between the variables.
Then, we can assess the fit of the model to the observed data, and evaluate the overall model’s explanatory power. It’s validity.</p>
<div id="model" class="section level3 unnumbered">
<h3>Model<a class="anchor" aria-label="anchor" href="#model"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="va">model_1</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">Evaluation =~ Q47 + Q48  + Q49</span></span>
<span><span class="st">Immersion =~ Q33 + Q35 + Q31 </span></span>
<span><span class="st">Embodiment =~ Q37 + Q39 + Q41</span></span>
<span><span class="st">Emotion =~ Q43</span></span>
<span><span class="st">Kind_of_experience =~ Immersion + Embodiment + Emotion</span></span>
<span><span class="st">Kind_of_experience ~ Quality </span></span>
<span><span class="st">Evaluation ~ Kind_of_experience</span></span>
<span><span class="st">Q1 ~ Evaluation + Q2'</span></span></code></pre></div>
<p>The SEM follows the syntax from the R-package <code>lavaan.</code>
The operator <code>=~</code> defines a confirmatory factor analysis, which is used to created a latent variable.
For example, <code>Evaluation</code> is constructed from Q47, Q48 and Q49.
Similarly, <code>Immerson</code>, <code>Embodiment</code> and <code>Emotion</code> are latent variables, and they all define <code>Kind_of_experience</code>.
The operator <code>~</code> defines a regression, which is used to relate a response variable to predictor variables.
<code>Quality</code> (as indicator) has a link to <code>Kind_of_experience</code> and <code>Kind_of_experience</code> to <code>Evaluation</code>.
Then, <code>Evaluation</code> and Q2 are predictors for <code>Q1</code>, which was called <code>Global_appreciation</code> in our DAG of figure <a href="chapListener.html#fig:chapListenerDAG1">3.2</a>.</p>
</div>
<div id="covariance-matrix" class="section level3 unnumbered">
<h3>Covariance matrix<a class="anchor" aria-label="anchor" href="#covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>Mathematically speaking, a SEM fits the covariance matrix <span class="math inline">\(\Sigma(\theta)\)</span> of a defined model with a covariance matrix <span class="math inline">\(\Sigma\)</span> of the data. In optimal fitting:
<span class="math display">\[
\Sigma = \Sigma(\theta),
\]</span>
where <span class="math inline">\(\theta\)</span> are the parameters of the model.</p>
<p>Our pruned questionnaire contains 12 questions of about 800 respondents who answered the questions. That gives us a 12x12 covariance matrix of questions (<span class="math inline">\(\Sigma\)</span>), and a 12x12 covariance matrix of questions embedded in a model with parameters, <span class="math inline">\(\Sigma(\theta)\)</span>.
Then, this <span class="math inline">\(\Sigma(\theta)\)</span> will be optimized to approach <span class="math inline">\(\Sigma\)</span>.
When optimization is successful, we obtained insight in the data from the viewpoint of the model, which was inspired by our theory of music appreciation.</p>
<p>Thus, rather than just putting all data unrelated in a box, our theory suggests that data are structured.
The data come from measurements using questions that relate to a theory.
Hence, it can be tested if that theory is justified by the data.</p>
</div>
<div id="cfa-and-regression" class="section level3 unnumbered">
<h3>CFA and regression<a class="anchor" aria-label="anchor" href="#cfa-and-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Let us just recall what we just said.
Structure among variables, as specified by the DAG, is defined by <em>confirmatory factor analysis (CFA)</em> and <em>regression.</em>
The <em>CFA</em> generates a new latent variable from a weighted sum of variables.
For example, the latent variable <code>Immersion</code> is generated from indicators Q33, Q35, Q31.
The <em>regression</em> associates a given variable with a weighted sum of given variables.
For example, Q1 (the response) is associated with <code>Evaluation</code> and Q2 (the predictors).
Recall that <code>Quality</code> is not really a latent variable anymore in this model because it was obtained by combining yes/no questions in a pre-processing stage. In contrast, <code>Kind_of_experience</code> and <code>Evaluation</code> can be considered genuine latent variables.</p>
</div>
</div>
<div id="dataset-and-sem" class="section level2" number="3.9">
<h2>
<span class="header-section-number">3.9</span> Dataset and SEM<a class="anchor" aria-label="anchor" href="#dataset-and-sem"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s now fit the SEM with our data. Recall that the goal of doing this fitting is to see whether there is indeed structure in our data, as defined in the model.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">semfit1</span> <span class="op">&lt;-</span> <span class="fu">lavaan</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lavaan/man/sem.html">sem</a></span><span class="op">(</span><span class="va">model_1</span>, data <span class="op">=</span> <span class="va">Data</span>, </span>
<span>                       group <span class="op">=</span> <span class="st">"Q57"</span>,  meanstructure <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><code>Lavaan</code> has a lot of bells and whistles. We will limit ourselves here to a simple fitting in which we use Q57 to divide the database in two parts, based on <em>Liking</em> and <em>Disliking</em>.
The two parts of our database reflect the fact that listeners filled in the same questionnaire twice. First for their preferred music, and then for a piece that they did not like.
When fitting the models, we obtain weights for the parameters that associate the variables.
However, whether the fitted model is acceptable depends on some tests that check the discrepancy between <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Sigma(\theta)\)</span>.</p>
<div id="measurement-invariance" class="section level3 unnumbered">
<h3>Measurement invariance<a class="anchor" aria-label="anchor" href="#measurement-invariance"><i class="fas fa-link"></i></a>
</h3>
<p>We use a technique called called <em>measurement invariance</em> to check the consistency across different groups or conditions simply by comparing the fit of several nested models that impose increasingly restrictive constraints on the parameters of the measurement model across groups. If measurement invariance is not established, differences in the observed scores between groups may be due to measurement bias rather than true differences in the underlying construct. Here we have a summary of the measurmement invariance.</p>
<p></p>
<pre><code>## ####################### Model Fit Indices ###########################
##             chisq  df pvalue rmsea   cfi   tli  srmr        aic        bic
## semfit1  902.093† 122   .000 .089  .849† .810  .083† 40633.042† 41042.366 
## semfit2  956.857  130   .000 .089† .840  .811† .085  40671.807  41038.044†
## semfit3 1125.185  136   .000 .095  .809  .783  .093  40828.134  41162.057</code></pre>
<p></p>
<p>Comparing the results of different models (semfit1, semfit2, and semfit3), we can see the following trends:</p>
<ul>
<li><p>Chi-square: All models have significant chi-square values (p &lt; .05), indicating that the models do not perfectly fit the data. However, this is often the case with large sample sizes, where even minor deviations from the model can lead to significant chi-square values.</p></li>
<li><p>RMSEA: All models have RMSEA values around .089-.095, which are within an acceptable range (typically below .08 for good fit), indicating reasonable fit.</p></li>
<li><p>CFI and TLI: All models have CFI values around .81-.85 and TLI values around .79-.81, suggesting a reasonable fit.</p></li>
<li><p>SRMR: Model semfit1 has the lowest SRMR (.083), indicating better fit in terms of standardized root mean square residual.</p></li>
<li><p>AIC and BIC: Model semfit1 has the lowest AIC value among the three models, indicating better parsimony.</p></li>
</ul>
<p>Based on these results, the conclusion might be that model semfit1 provides the best overall fit to the data among the three models tested. However, given the minor differences among the three models, measurement invariance can be assumed.</p>
</div>
<div id="parameters" class="section level3 unnumbered">
<h3>Parameters<a class="anchor" aria-label="anchor" href="#parameters"><i class="fas fa-link"></i></a>
</h3>
<p>Here we inspect the estimated parameters for the latent variables and the regression:</p>
<p></p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">semfit1</span>, standardized <span class="op">=</span> <span class="cn">TRUE</span>, rsquare <span class="op">=</span> <span class="cn">TRUE</span>, ci <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="va">reg</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">Group 1 [Liking]:</span></span>
<span><span class="st"></span></span>
<span><span class="st">Latent Variables:</span></span>
<span><span class="st">                        Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper   Std.lv  Std.all</span></span>
<span><span class="st">  Evaluation =~                                                                                </span></span>
<span><span class="st">    Q47                    1.000                               1.000    1.000    0.315    0.674</span></span>
<span><span class="st">    Q48                    0.848    0.114    7.422    0.000    0.624    1.072    0.267    0.383</span></span>
<span><span class="st">    Q49                   -0.368    0.054   -6.756    0.000   -0.474   -0.261   -0.116   -0.337</span></span>
<span><span class="st">  Immersion =~                                                                                 </span></span>
<span><span class="st">    Q33                    1.000                               1.000    1.000    0.671    0.783</span></span>
<span><span class="st">    Q35                    0.702    0.050   13.968    0.000    0.604    0.801    0.471    0.600</span></span>
<span><span class="st">    Q31                    0.856    0.058   14.747    0.000    0.742    0.969    0.574    0.649</span></span>
<span><span class="st">  Embodiment =~                                                                                </span></span>
<span><span class="st">    Q37                    1.000                               1.000    1.000    0.896    0.727</span></span>
<span><span class="st">    Q39                    0.957    0.104    9.205    0.000    0.753    1.161    0.857    0.672</span></span>
<span><span class="st">    Q41                    0.408    0.052    7.860    0.000    0.306    0.509    0.365    0.381</span></span>
<span><span class="st">  Emotion =~                                                                                   </span></span>
<span><span class="st">    Q43                    1.000                               1.000    1.000    0.681    1.000</span></span>
<span><span class="st">  Kind_of_experience =~                                                                        </span></span>
<span><span class="st">    Immersion              1.000                               1.000    1.000    0.870    0.870</span></span>
<span><span class="st">    Embodiment             0.602    0.094    6.412    0.000    0.418    0.786    0.393    0.393</span></span>
<span><span class="st">    Emotion                0.688    0.075    9.156    0.000    0.540    0.835    0.589    0.589</span></span>
<span><span class="st"></span></span>
<span><span class="st">Regressions:</span></span>
<span><span class="st">                       Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper   Std.lv  Std.all</span></span>
<span><span class="st">  Evaluation ~                                                                                </span></span>
<span><span class="st">    Kind_of_exprnc        0.322    0.040    8.028    0.000    0.244    0.401    0.597    0.597</span></span>
<span><span class="st">  Kind_of_experience ~                                                                        </span></span>
<span><span class="st">    Quality               0.089    0.027    3.300    0.001    0.036    0.142    0.152    0.141</span></span>
<span><span class="st">  Q1 ~                                                                                        </span></span>
<span><span class="st">    Evaluation            1.382    0.160    8.630    0.000    1.068    1.696    0.435    0.497</span></span>
<span><span class="st">    Q2                   -0.015    0.046   -0.318    0.750   -0.105    0.075   -0.015   -0.010</span></span>
<span><span class="st">'</span></span></code></pre></div>
<p>
All latent variables, except Q2, fit well.
This can be seen in the P(&gt;|z|) or Std.all columns.
When looking at the regression we see that Q2 doesn’t play any role for the specification of Q1.</p>
<p></p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span> <span class="co"># summary(semfit1, standardized = TRUE, rsquare = TRUE, ci = T)</span></span>
<span><span class="va">reg</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">Group 2 [Disliking]:</span></span>
<span><span class="st"></span></span>
<span><span class="st">Latent Variables:</span></span>
<span><span class="st">                        Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper   Std.lv  Std.all</span></span>
<span><span class="st">  Evaluation =~                                                                                </span></span>
<span><span class="st">    Q47                    1.000                               1.000    1.000    0.568    0.787</span></span>
<span><span class="st">    Q48                    0.988    0.049   20.044    0.000    0.892    1.085    0.561    0.761</span></span>
<span><span class="st">    Q49                   -0.742    0.067  -11.033    0.000   -0.874   -0.610   -0.421   -0.420</span></span>
<span><span class="st">  Immersion =~                                                                                 </span></span>
<span><span class="st">    Q33                    1.000                               1.000    1.000    0.617    0.837</span></span>
<span><span class="st">    Q35                    1.027    0.040   25.803    0.000    0.949    1.105    0.633    0.854</span></span>
<span><span class="st">    Q31                    0.775    0.059   13.041    0.000    0.659    0.892    0.478    0.467</span></span>
<span><span class="st">  Embodiment =~                                                                                </span></span>
<span><span class="st">    Q37                    1.000                               1.000    1.000    0.766    0.901</span></span>
<span><span class="st">    Q39                    0.904    0.044   20.712    0.000    0.818    0.989    0.692    0.800</span></span>
<span><span class="st">    Q41                    0.413    0.059    6.989    0.000    0.298    0.529    0.317    0.261</span></span>
<span><span class="st">  Emotion =~                                                                                   </span></span>
<span><span class="st">    Q43                    1.000                               1.000    1.000    1.287    1.000</span></span>
<span><span class="st">  Kind_of_experience =~                                                                        </span></span>
<span><span class="st">    Immersion              1.000                               1.000    1.000    0.940    0.940</span></span>
<span><span class="st">    Embodiment             0.917    0.058   15.852    0.000    0.804    1.031    0.694    0.694</span></span>
<span><span class="st">    Emotion                0.664    0.086    7.766    0.000    0.497    0.832    0.299    0.299</span></span>
<span><span class="st"></span></span>
<span><span class="st">Regressions:</span></span>
<span><span class="st">                       Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper   Std.lv  Std.all</span></span>
<span><span class="st">  Evaluation ~                                                                                </span></span>
<span><span class="st">    Kind_of_exprnc        0.840    0.051   16.573    0.000    0.740    0.939    0.857    0.857</span></span>
<span><span class="st">  Kind_of_experience ~                                                                        </span></span>
<span><span class="st">    Quality               0.205    0.017   12.108    0.000    0.172    0.238    0.354    0.453</span></span>
<span><span class="st">  Q1 ~                                                                                        </span></span>
<span><span class="st">    Evaluation            1.517    0.109   13.914    0.000    1.303    1.731    0.861    0.523</span></span>
<span><span class="st">    Q2                   -0.142    0.044   -3.190    0.001   -0.229   -0.055   -0.142   -0.098</span></span>
<span><span class="st">'</span></span></code></pre></div>
<p></p>
<p>As far as the second part of our dataset is concerned, all latent variables fit well.
Again, Q2 doesn’t contribute much to the global result.</p>
<p>Overall, the model suggests that <code>Evaluation</code> is a relevant predictor for Q1.
Moreover, <code>Evaluation</code> is predicted by <code>Kind_of_experience</code> and
<code>Immersion</code> is the strongest contributor in Liking and Disliking groups.</p>
<p>In the Liking group, <code>Emotion</code> contributes more than <code>Embodiment</code>, while in Disliking, <code>Embodiment</code> contributes more than <code>Emotion.</code>
<code>Quality</code> has less an impact in the Liking group, compared to the Disliking group.</p>
<p>Overall, in the Liking group <code>Immersion</code> and <code>Emotion</code> are strong predictors, while in the Disliking group, <code>Immersion</code> and <code>Embodiment</code> and <code>Quality</code> are strong. When they like the music, subjects are more emotionally touched. When they dislike the music, subjects seem to be overall less emotionally touched. They refer more consistently to the quality of the music as source of disliking.</p>
</div>
</div>
<div id="predictions" class="section level2" number="3.10">
<h2>
<span class="header-section-number">3.10</span> Predictions<a class="anchor" aria-label="anchor" href="#predictions"><i class="fas fa-link"></i></a>
</h2>
<p>In this and the following sections, we show that the model can be used for predicting.</p>
<div id="j.s.-bach" class="section level3 unnumbered">
<h3>J.S. Bach<a class="anchor" aria-label="anchor" href="#j.s.-bach"><i class="fas fa-link"></i></a>
</h3>
<p>In a first example, we’ll predict the listeners’ overall appreciation (Q1) of Bach pieces.
Can we predict Q1 for Bach pieces, given a SEM trained with a dataset where Bach pieces have not been included?
This can only happen if the SEM captures necessary information from other pieces, so that it can predict the Q1 for new pieces using the indicators Q47, Q48, Q33, Q35, Q31 and Quality. In other words, SEM has to generalize its input to Q1 so that it applies to Bach.</p>
<p>To figure that out, a distinction is made between <em>training-data</em> and <em>test-data</em>.
The training-data contain all data except the answers of 157 listeners who said something about Bach.
The test-data contain only Bach pieces.
Then, a SEM is trained with the training-data, and predictions are regenerated.</p>
<div class="inline-table"><table class="table table-striped" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:chapListenerBachPrediction">Table 3.2: </span>Prediction of Bach pieces, versus original data
</caption>
<thead><tr>
<th style="text-align:right;">
Q1_predict
</th>
<th style="text-align:right;">
Q1_data
</th>
<th style="text-align:left;">
LD
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
11.86
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
<tr>
<td style="text-align:right;">
12.04
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
<tr>
<td style="text-align:right;">
12.55
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
<tr>
<td style="text-align:right;">
10.67
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
<tr>
<td style="text-align:right;">
12.84
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
<tr>
<td style="text-align:right;">
13.54
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Liking
</td>
</tr>
</tbody>
</table></div>
<p>Figure <a href="chapListener.html#fig:chapListenerBachpred1">3.7</a> shows the prediction of Bach appreciation in <code>Q1_prediction</code> and the original Bach appreciation data in <code>Q1_data</code> (cor =.95). The horizontal axis shows the true Q1, the answers given by the participants. The vertical axis shows the predicted Q1, the answers generated by the model that doesn’t know Bach. The regression lines show both a linear regression (straight line) and a loess or Locally Estimated Scatterplot Smoothing (sigmoid line).
To generate the figure, see the code in <code>Code/chapListener/chapListener_05_ModelPlotting.R</code>.</p>
<div class="figure">
<span style="display:block;" id="fig:chapListenerBachpred1"></span>
<img src="Figures/chapListener_Bach_pred1_nonscaled.png" alt="Prediction of global appreciation of Bach pieces" width="80%"><p class="caption">
Figure 3.7: Prediction of global appreciation of Bach pieces
</p>
</div>
<p>The model gives a rather accurate prediction of the global appreciation of Bach’s pieces.
Interestingly, Bach’s pieces are not always liked.
A more in depth analysis would be needed to figure out whether this depends on particular music pieces.</p>
</div>
<div id="attributed-affects" class="section level3 unnumbered">
<h3>Attributed affects<a class="anchor" aria-label="anchor" href="#attributed-affects"><i class="fas fa-link"></i></a>
</h3>
<p>In a second example, we look at latent variables from the viewpoint of sad, relaxing, aggressive and happy music.
These categories were introduced in figure <a href="chapListener.html#fig:chapListenerArousalValenceLikeDislike">3.5</a>.
After prediction, figure <a href="chapListener.html#fig:chapListenerArousalValencepred">3.8</a> is created.
It shows the distribution of 50 dots per latent variable and these latent variables are scaled from -2 to 2.
The distributions reflect what each latent variable contributes to Q1 for sad, relaxing, aggressive and happy music.
<code>Evaluation</code> and <code>Experience</code> display similar distributions, meaning that the appraisal about experiences depends on whether experiences have a high degree of immersion.</p>
<p>The figure reveals that the distribution of relaxing, aggressive, and happy tends towards an unipolar scoring, with 96%, 82% and 78% of the counted dots at one side of <code>Evaluation.</code> This scoring is in agreement with the valence.
High valence (relaxing and happy) get high scores, low valence (aggressive) gets low scores.
However, the distribution of sad tends to be bi-polar with a balance of 62% having a negative evaluation.
That means that 38% of the listeners, with a decisive opinion about the arousal-valence questions, had attributed high <code>Evaluation</code> to sad music and thus a high overall appreciation (Q1) because both are strongly correlated.</p>
<div class="figure">
<span style="display:block;" id="fig:chapListenerArousalValencepred"></span>
<img src="Figures/chapListener_ArousalValence_pred.png" alt="Latent variables from SEM modelling and distributions for the category of happy,  aggressive, relaxing, and sad, as attributed by listeners" width="100%"><p class="caption">
Figure 3.8: Latent variables from SEM modelling and distributions for the category of happy, aggressive, relaxing, and sad, as attributed by listeners
</p>
</div>
</div>
</div>
<div id="discussion" class="section level2" number="3.11">
<h2>
<span class="header-section-number">3.11</span> Discussion<a class="anchor" aria-label="anchor" href="#discussion"><i class="fas fa-link"></i></a>
</h2>
<p>Overall, the statistics supports our proposed theory of music appreciation.
Music appreciation rest mainly on the evaluation of pleasurable experiences and
the attribution of quality plays a role depending on whether music is liked or disliked.</p>
<p>The main findings are:</p>
<ul>
<li><p>When a listener likes the music, then quality scores low, compared to the situation when a listener dislikes the music. This can be explained by the fact that the listener had anticipated certain qualities in the music. In liked music, the qualities agree with the anticipation and therefore, their role is less prominent than when the qualities do not agree with the anticipation. State otherwise, when people dislike the music, it is attributed to the musical qualities, and not to their anticipation.</p></li>
<li><p>The causes of appreciation are experiences. Immersion appears to be the most important consistent predictor, while embodiment and emotion have different roles depending on whether music is liked or not. This is somewhat surprising because the literature puts quite a lot of emphasis on emotion. Here we show that the experience of unity and absorption are more consistent and strong predictors, compared to felt emotions.</p></li>
</ul>
<p>The connection between quality and experience/evaluation may be intricate.
A listener may find that the music is of high quality, for example due to its composition, its performance, and so on, but that the music doesn’t appeal to a pleasant feeling when played over the radio. Probably, it would get a low rating for appreciation.
Hhowever, in our dataset we don’t have examples of dis-accordance between attributed quality and experience/evaluation.
This may be due to the fact that listeners were asked to give an example of music they dislike when hearing over the radio, and then finding a rationale for the disliking.</p>
<p>If listeners would have been asked to select a piece with high quality, and then rationalize whether that piece would be pleased when hearing over the radio, the situation might have been different because a context is added.
The reason why somebody doesn’t like to hear music over the radio may not be due to intrinsic musical qualities, but to context. This aspect probably needs further clarification in follow-up studies.</p>
<p>Finally, what happens when listeners hear the same music repeatedly? Can they still experience reward when the anticipation-reward-motivation mechanism becomes saturated? The theory suggests an affirmative answer.
Listeners may become entrained by the music. Similar to a locomotive pulling wagons, music propels the listener to a point where reward can be consistently experienced. The crux lies in its dynamic binding structure, known as <em>sonic moving forms</em> and their ability to dynamically shape anticipation through the melodic line, harmonic progression, rhythmic flow, tension, and resolution. Being entrained by the music, embodied experiences are anticipative. This feeling is potent, fostering a sense of control that is rewarding and, thus, pleasurable (see the illusion of reversed causality explained in chapter <a href="chapTheory.html#chapTheory">1</a>).</p>
</div>
<div id="conclusion-2" class="section level2" number="3.12">
<h2>
<span class="header-section-number">3.12</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-2"><i class="fas fa-link"></i></a>
</h2>
<p>In this chapter, we probed listeners to reflect on what music does to them. Using a questionnaire, we refined a theory of music appreciation. Based on neurobiology, marketing and musicology, a causal model was tested in a structural equation modelling approach using data from this questionnaire.</p>
<p>The model suggests that music can cause experiences in listeners and that immersion is an important experience contributing to appreciation.
Can it be linked with our dynamic concept of self-augmented interactions?
The latter indeed suggests that experiences occur during the interaction with music, rather than after the interaction with music.</p>
<p>Related to our theory of self-augmented interaction, expressed in chapter <a href="chapTheory.html#chapTheory">1</a>, the theory of appreciation focuses more on the reflective aspect, involving an evaluation of the state reached. It is likely that self-augmented interactions will lead to highly appreciated music. And rationalizations about appreciation may well be relevant for understanding self-augmentation. Again, in appreciation theory we are dealing with an off-line situation, whereas our self-augmentation theory attempts a grasping the on-line situation.</p>
<p>In the chapters that follow, we focus less on appreciation, and less on experiences even. Instead, we look at music interactions from the viewpoint of performance.
In that sense, we put more emphasis on encoders (dancers, musicians) than on decoders (listeners).</p>
<!-- ## References -->
<!-- Jacobs et al. (in preparation) -->
<!-- McElreath (2020) -->

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="chapModelling.html"><span class="header-section-number">2</span> Modelling</a></div>
<div class="next"><a href="chapDancer.html"><span class="header-section-number">4</span> Dancer synchronization</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <h2>Provide your feedback</h2>
    <!--<p>Now is a great time to provide feedback</p>-->
        <ul class="list-unstyled">
<!--<li><a href="https://forms.gle/nq9RmbxJyZXQgc948">Provide feedback (5 min)</a></li>--><li><a href="https://www.ugent.be/lw/kunstwetenschappen/ipem/en/services/asil">art and science interaction lab</a></li>
        </ul>
<hr>
<nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapListener"><span class="header-section-number">3</span> Listener appreciation</a></li>
<li><a class="nav-link" href="#workflow"><span class="header-section-number">3.1</span> Workflow</a></li>
<li><a class="nav-link" href="#theory"><span class="header-section-number">3.2</span> Theory</a></li>
<li>
<a class="nav-link" href="#causal-model"><span class="header-section-number">3.3</span> Causal model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#directed-acyclic-graph-dag">Directed acyclic graph (DAG)</a></li>
<li><a class="nav-link" href="#confounding-variables">Confounding variables</a></li>
</ul>
</li>
<li><a class="nav-link" href="#questionnaire"><span class="header-section-number">3.4</span> Questionnaire</a></li>
<li><a class="nav-link" href="#survey-and-data"><span class="header-section-number">3.5</span> Survey and data</a></li>
<li>
<a class="nav-link" href="#inspect-the-data"><span class="header-section-number">3.6</span> Inspect the data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#appreciation">Appreciation</a></li>
<li><a class="nav-link" href="#participants">Participants</a></li>
<li><a class="nav-link" href="#affect-attribution">Affect attribution</a></li>
</ul>
</li>
<li><a class="nav-link" href="#preparing-analysis"><span class="header-section-number">3.7</span> Preparing analysis</a></li>
<li>
<a class="nav-link" href="#structural-equation-modelling"><span class="header-section-number">3.8</span> Structural equation modelling</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#model">Model</a></li>
<li><a class="nav-link" href="#covariance-matrix">Covariance matrix</a></li>
<li><a class="nav-link" href="#cfa-and-regression">CFA and regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#dataset-and-sem"><span class="header-section-number">3.9</span> Dataset and SEM</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#measurement-invariance">Measurement invariance</a></li>
<li><a class="nav-link" href="#parameters">Parameters</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#predictions"><span class="header-section-number">3.10</span> Predictions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#j.s.-bach">J.S. Bach</a></li>
<li><a class="nav-link" href="#attributed-affects">Attributed affects</a></li>
</ul>
</li>
<li><a class="nav-link" href="#discussion"><span class="header-section-number">3.11</span> Discussion</a></li>
<li><a class="nav-link" href="#conclusion-2"><span class="header-section-number">3.12</span> Conclusion</a></li>
</ul>
      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mlIpem/ExploringMusicInteractionWithR/blob/main/04_chapListener.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mlIpem/ExploringMusicInteractionWithR/edit/main/04_chapListener.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Exploring music interactions</strong>" was written by Marc Leman. It was last built on 2024-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
